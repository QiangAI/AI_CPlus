{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据访问"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## item函数\n",
    "- 直接把标量Tensor转换为Python类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# t = torch.Tensor([\n",
    "#     [1, 2, 3],\n",
    "#     [4, 5, 6],\n",
    "#     [7, 8, 9]\n",
    "# ])\n",
    "t = torch.Tensor([1])\n",
    "print(t.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data属性与detach函数\n",
    "\n",
    "- data与原Tensor共享Storage，但是属性会改变：requires_grad会变为False。\n",
    "    - 可以通过data修改原数据，修改后，在运行时不会检测，然后可能会产生错误\n",
    "- detach()函数返回的是从计算图中剥离出来的Tensor，requires_grad=False。\n",
    "    - 通过detach()函数返回的张量修改数据，这种修改在运行的时候，会先检测是否修改，从而在运行前触发错误。\n",
    "    \n",
    "- 注意：\n",
    "    - 关于Tensor的检测实际与图跟踪有关，这个在Torch中提供了上下文管理来处理，个人喜欢使用上下文管理器，用来管理作用在Tensor上的各种运算操作跟踪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "False\n",
      "tensor([[88.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# data的不安全说明\n",
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "t.requires_grad=True\n",
    "print(t.data)\n",
    "print(t.data.requires_grad)\n",
    "t.data[0,0] =88                    # 这种修改在运行时可能导致致命错误\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "False\n",
      "tensor([[88.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[88.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.],\n",
       "        [ 7.,  8.,  9.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### detach的安全说明（修改的时候直接检测，就是不准修改了，这样安全）\n",
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "t.requires_grad=True\n",
    "print(t.detach())\n",
    "print(t.detach().requires_grad)\n",
    "t.detach()[0,0] =88                    # 这种修改在运行时可能导致致命错误\n",
    "print(t)\n",
    "t.detach().resize_(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9661e-01, 1.0499e-01, 4.5177e-02],\n",
      "        [1.7663e-02, 6.6480e-03, 2.4665e-03],\n",
      "        [9.1017e-04, 3.3522e-04, 1.2337e-04]])\n",
      "tensor([[-7.6558e+03,  2.0999e-01,  9.0353e-02],\n",
      "        [ 3.5325e-02,  1.3296e-02,  4.9329e-03],\n",
      "        [ 1.8203e-03,  6.7045e-04,  2.4673e-04]])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# data的不安全说明\n",
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "t.requires_grad=True\n",
    "y = t.sigmoid()\n",
    "y_ = y.sum() \n",
    "r = y_.backward(retain_graph=True)   # backward本身不会返回值 r = Nones\n",
    "print(t.grad)\n",
    "\n",
    "# 修改数据\n",
    "y.data[0,0]=88    # 不检测错误，但可能已经有错误（不允许在计算中途修改）\n",
    "y_.backward()   # backward本身不会返回值 r = Nones\n",
    "print(t.grad)\n",
    "print(t.data[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9661e-01, 1.0499e-01, 4.5177e-02],\n",
      "        [1.7663e-02, 6.6480e-03, 2.4665e-03],\n",
      "        [9.1017e-04, 3.3522e-04, 1.2337e-04]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3, 3]], which is output 0 of SigmoidBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f1b6b9abcc6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 修改数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m88\u001b[0m    \u001b[0;31m# 检测错误，修改数据就会报错，这样最终结果安全。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0my_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# backward本身不会返回值 r = Nones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3, 3]], which is output 0 of SigmoidBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# data的不安全说明\n",
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "t.requires_grad=True\n",
    "y = t.sigmoid()\n",
    "y_ = y.sum() \n",
    "r = y_.backward(retain_graph=True)   # backward本身不会返回值 r = Nones\n",
    "print(t.grad)\n",
    "\n",
    "# 修改数据\n",
    "y.detach()[0,0]=88    # 检测错误，修改数据就会报错，这样最终结果安全。\n",
    "y_.backward()   # backward本身不会返回值 r = Nones\n",
    "print(t.grad)\n",
    "print(t.data[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy函数\n",
    "\n",
    "- 把Tensor转换为Numpy的ndarray格式。\n",
    "- 这种转换大部分没有意义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(t.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## storage/storage_offset/storage_type函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以直接返回Tensor的数据存储对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.0\n",
      " 2.0\n",
      " 3.0\n",
      " 4.0\n",
      " 5.0\n",
      " 6.0\n",
      " 7.0\n",
      " 8.0\n",
      " 9.0\n",
      "[torch.FloatStorage of size 9]\n",
      "0\n",
      "4\n",
      "<class 'torch.FloatStorage'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(t.storage())\n",
    "print(t.storage_offset())\n",
    "print(t[1,1].storage_offset())\n",
    "print(t.storage_type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stride函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 返货Tensor的对Storage的行列的间隔步长，按照维数指定，维度由0，1，2，...指定\n",
    "    - 0行\n",
    "    - 1列\n",
    "    \n",
    "    \n",
    "- 注意：\n",
    "    - 使用负数指定逆序的维数，-1表示最后一个维度，-2就是倒数第二个维度，\n",
    "    - 步长的单位是Storage的元素长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(t.stride())   # 返回所有维度\n",
    "print(t.stride(1))   # 返回第二维读步长\n",
    "print(t.stride(0))   \n",
    "print(t.stride(-1))   \n",
    "print(t.stride(-2))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## as_strided函数\n",
    "\n",
    "- 按照指定的步长返回新的Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [6., 7.]])\n",
      "tensor([[ 1.,  2., 88.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "s = t.as_strided((2, 2), (3, 1), storage_offset=2)   # 返回的张量与原来的Storage共享存储空间\n",
    "print(s)\n",
    "s[0,0]=88   \n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## view与view_as函数\n",
    "\n",
    "- as_strided函数特例版本，不需要offset，不需要stride\n",
    "- 需要view前后的size一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "s = t.view((1,9))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "v = torch.Tensor(1, 9)\n",
    "\n",
    "s = t.view_as(v)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to与to_\\*\\*函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to系列函数有：\n",
    "    - to\n",
    "        - dtype 与 device转换\n",
    "    - to_dense/ to_sparse\n",
    "        - 稀疏矩阵与稠密矩阵之间转换（转换的是layout格式）\n",
    "    - to_mkldnn\n",
    "        - cpu加速\n",
    "    - to_list\n",
    "        - 转换为python类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]], dtype=torch.int32)\n",
      "tensor(indices=tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
      "                       [0, 1, 2, 0, 1, 2, 0, 1, 2]]),\n",
      "       values=tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]),\n",
      "       size=(3, 3), nnz=9, layout=torch.sparse_coo)\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]], layout=torch._mkldnn)\n",
      "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(t.to(dtype=torch.int32, device=torch.device(\"cpu:0\")))    # 还可以使用device，根据cpu个数来，只有一个就只能是0\n",
    "print(t.to_sparse())     # 稠密矩阵调用这个\n",
    "# print(t.to_dense())  # 稀疏Tensor就调用这个\n",
    "print(t.to_mkldnn())\n",
    "print(t.tolist())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## type与type_as函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- type与type_as主要是类型转换。\n",
    "    - type_as是使用参数的Tensor类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(t.type(torch.float32))  \n",
    "\n",
    "t.type_as(t.type(torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类型转换系列函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]], dtype=torch.int32)\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]], dtype=torch.uint8)\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]], dtype=torch.int8)\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]], dtype=torch.int16)\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]], dtype=torch.float16)\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]], dtype=torch.float64)\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "print(t.int())\n",
    "print(t.long())\n",
    "print(t.byte())\n",
    "print(t.char())\n",
    "print(t.short())\n",
    "print(t.half())\n",
    "print(t.float())\n",
    "print(t.double())\n",
    "print(t.bool())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reshape与resize函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这个系列函数有：\n",
    "    - reshape：不改变Tensor的元素个数\n",
    "    - reshape_as：使用已知Tensor作为参照\n",
    "    - resize：改变元素个数\n",
    "    - resize_as_：使用已知Tensor作为参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "print(t.reshape(1, 9))\n",
    "print(t.resize_(2, 2))   # 按照顺序来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_ptr函数\n",
    "\n",
    "- 返回第一个元素的地址，一般没有什么意义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140604559708032\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "print(t.data_ptr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dense_dim与sparse_dim函数\n",
    "- 返回稀疏与稠密矩阵维度\n",
    "- 这两个函数都是稀疏矩阵使用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "print(t.to_sparse().sparse_dim())\n",
    "print(t.to_sparse().dense_dim()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## element_size函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 元素大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.LongTensor(2, 3)\n",
    "t2 = torch.DoubleTensor(3, 2)\n",
    "print(t1.element_size(),  t2.element_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_device函数\n",
    "\n",
    "- 获取设备\n",
    "    - 对CPU来说，应该抛出异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.LongTensor(2, 3)\n",
    "print(t1.cpu().get_device())    # 返回-1，其实应该是抛出异常。\n",
    "print(t1.get_device())    # 返回-1，其实应该是抛出异常。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nelement函数\n",
    "\n",
    "- 返回所有元素个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.LongTensor(2, 3)\n",
    "print(t1.nelement())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## flatten函数\n",
    "\n",
    "- 与numpy的flat函数一样，把Tensor的layout从多维变成一维。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.LongTensor(2, 3)\n",
    "print(t1.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dequantize函数\n",
    "\n",
    "- Tensor去量化\n",
    "    - 量化是一种离散化的技术，可以用于数据压缩等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.FloatTensor([0.0, 10.50,  11.50, 11.45])    # 被离散化\n",
    "print(t1.is_quantized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([ 0.0000, 10.4000, 11.6000, 11.4000], size=(4,), dtype=torch.qint8,\n",
      "       scale=0.2, zero_point=0)\n",
      "tensor([ 0.0000, 10.4000, 11.6000, 11.4000])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 线性量化函数，这个函数在卷积运算中使用。\n",
    "q_t1= torch.quantize_linear(t1,  0.2,  0, torch.qint8)\n",
    "print(q_t1.is_quantized)\n",
    "print(q_t1.data)\n",
    "\n",
    "print(q_t1.dequantize())\n",
    "print(q_t1.dequantize().is_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## values函数\n",
    "\n",
    "- 返回稀疏Tensor的数据;\n",
    "    - 矩阵类型必须是coalesced 稀疏矩阵(聚接后的稀疏矩阵的值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4., 5.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "i = torch.tensor(\n",
    "    [[0, 1, 1],\n",
    "     [2, 0, 2]])\n",
    "v = torch.tensor([3, 4, 5], dtype=torch.float32)\n",
    "sp = torch.sparse_coo_tensor(i, v)  \n",
    "\n",
    "print(sp.coalesce().values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indices函数\n",
    "\n",
    "- 这个函数只争对torch.sparse_coo布局的稀疏矩阵。\n",
    "- 返回值得索引，见values函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 1],\n",
      "        [2, 0, 2]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "i = torch.tensor(\n",
    "    [[0, 1, 1],\n",
    "     [2, 0, 2]])\n",
    "v = torch.tensor([3, 4, 5], dtype=torch.float32)\n",
    "sp = torch.sparse_coo_tensor(i, v)  \n",
    "\n",
    "print(sp.coalesce().indices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## squeeze_/squeeze与unsqueeze/unsqueeze_函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 矩阵降维（减少维数）\n",
    "    - 前提是只有一个元素的向量，可以直接转化为标量，从而实现降维。\n",
    "    - 带后缀下划线的函数，影响被操作的Tensor，同时返回操作后的Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 4.])\n",
      "tensor([[2., 3., 4.]])\n",
      "tensor([[2.],\n",
      "        [3.],\n",
      "        [4.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.Tensor([\n",
    "    [2],[3],[4]\n",
    "])\n",
    "print(t1.squeeze())\n",
    "print(t1.squeeze().unsqueeze(0))   # 指定扩大哪一维\n",
    "print(t1.squeeze().unsqueeze(1))    # 不能取大于等于2的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 4.])\n",
      "tensor([2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.Tensor([\n",
    "    [2],[3],[4]\n",
    "])\n",
    "print(t1.squeeze_())\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
