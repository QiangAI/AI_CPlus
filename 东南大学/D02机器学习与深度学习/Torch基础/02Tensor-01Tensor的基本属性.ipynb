{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 回顾\n",
    "\n",
    "- 因为Torch的核心是Tensor，Tensor的数据由Storage管理，所以这两个类的关系搞清楚，就可以使用Tensor了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的Python构造器定义如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    Tensor.__init__(torch.device device)\n",
    "    Tensor.__init__(torch.Storage storage)\n",
    "    Tensor.__init__(Tensor other)\n",
    "    Tensor.__init__(tuple of ints size, torch.device device)\n",
    "    Tensor.__init__(object data, torch.device device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage的Python构造器定义如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    FloatStorage.__init__() no arguments\n",
    "    FloatStorage.__init__(int size)\n",
    "    FloatStorage.__init__(Sequence data)\n",
    "    FloatStorage.__init__(torch.FloatStorage view_source)\n",
    "    FloatStorage.__init__(torch.FloatStorage view_source, int offset)\n",
    "    FloatStorage.__init__(torch.FloatStorage view_source, int offset, int size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关于Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor与Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 实际上按照Python一贯的思路，会提供很多函数来替代构造器的使用，这样做有两个原因：\n",
    "    - 个性化，方便，简单；\n",
    "    - 使用工厂模式来创建对象，符合软件的常见设计模式，Python大量采用；\n",
    "        - 今后不要动不动就说面向对象最好，最方便。最直观，最方便的还是函数，拿来就用，不需要构建对象才能使用。\n",
    "        \n",
    "- Torch号称是GPU版本的Numpy，Numpy有的Tensor都有，所以按照Numpy的思路，在构建好对象后，有三大块功能是需要数理下的，掌握这三大基础功能，后面的内容就容易理解：\n",
    "    1. 基本属性\n",
    "        - 了解对象的内存与数据结构\n",
    "    2. 基本操作\n",
    "        - 数据进出\n",
    "    3. 数学运算\n",
    "        - 构建数据对象的最终目的就是计算；\n",
    "            - 计算的类别很多，基本数学运算，随机采样，线性代数的矩阵运算，统计计算，......\n",
    "            - 这里先明白基本的数学运算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的官方文档结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch\n",
    "        Tensors\n",
    "                Creation Ops\n",
    "                Indexing, Slicing, Joining, Mutating Ops\n",
    "                Generators\n",
    "        Random sampling\n",
    "                In-place random sampling\n",
    "                Quasi-random sampling\n",
    "                Serialization\n",
    "                Parallelism\n",
    "                Locally disabling gradient computation\n",
    "        Math operations\n",
    "                Pointwise Ops\n",
    "                Reduction Ops\n",
    "                Comparison Ops\n",
    "                Spectral Ops\n",
    "                Other Operations\n",
    "                BLAS and LAPACK Operations\n",
    "        Utilities\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这里先搞定Tensor本身的基本属性与操作\n",
    "    - 基本属性（从C/C++文档对应）\n",
    "    - 基本操作\n",
    "        - Indexing（索引访问操作）\n",
    "        - Slicing （切片访问操作【是索引的批量级升级版本】）\n",
    "        - Joining（数据组合与合并）\n",
    "        - Mutating Ops（数据访问：索引与切片的函数版本）\n",
    "    - 数学运算：\n",
    "        - Pointwise Ops（元素运算）\n",
    "        - Reduction Ops（降维运算）\n",
    "        - Comparison Ops（比较运算）\n",
    "        - Spectral Ops（谱运算）\n",
    "        - Other Operations（其他运算）\n",
    "        - BLAS and LAPACK Operations（线性代数运算）\n",
    "            1. BLAS\n",
    "                - Basic Linear Algebra Subprograms（Fortran语言编写，Fortran史上经典古老的数学计算语言）；\n",
    "            2. LAPACK\n",
    "                - Linear Algebra Package，底层使用的也是BLAS；\n",
    "            3. ATLAS\n",
    "                - Automatically Tuned Linear Algebra Software；\n",
    "            4. OpenBLAS：\n",
    "                - 在编译时根据目标硬件进行优化，生成运行效率很高的程序或者库。OpenBLAS的优化是在编译时进行的，所以其运行效率一般比ATLAS要高。因此OpenBLAS对硬件的依赖比较高，换一个硬件平台可能会重新进行编译。\n",
    "            5. cuBLAS与ACML：\n",
    "                - Intel的MKL和AMD的ACML都是在BLAS的基础上，针对自己特定的CPU平台进行针对性的优化加速。以及NVIDIA针对GPU开发的cuBLAS。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor的基本属性与属性函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 先构建一个张量（Tensor）使用；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t_vector = torch.LongTensor(\n",
    "    data= [1, 2, 3, 4, 5]\n",
    ")\n",
    "print(t_vector)\n",
    "t_matrix = torch.LongTensor(\n",
    "    data= [\n",
    "        [1, 2, 3, 4],\n",
    "        [5, 6, 7, 8]\n",
    "    ]\n",
    ")\n",
    "print(t_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 属性-T\n",
    "\n",
    "- 返回Tensor的转置；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n",
      "tensor([[1, 5],\n",
      "        [2, 6],\n",
      "        [3, 7],\n",
      "        [4, 8]])\n"
     ]
    }
   ],
   "source": [
    "print(t_vector.T)    # 向量转置还是本身，不产生转置效果\n",
    "print(t_matrix.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 属性-data\n",
    "\n",
    "- 返回张量的数据, 返回的也是张量，就是张量本身；\n",
    "    - 返回不同的id；\n",
    "    - 共享同一个Stroage；\n",
    "    - 但是data返回的数据状态改变：require s_grad = False，就是不能求导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n",
      "<class 'torch.Tensor'>\n",
      "4481379352 4510253344\n"
     ]
    }
   ],
   "source": [
    "# 地址不同\n",
    "print(t_vector.data)\n",
    "print(type(t_vector.data))\n",
    "print(id(t_vector), id(t_vector.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2, 88,  4,  5])\n"
     ]
    }
   ],
   "source": [
    "# 数据相互影响\n",
    "d = t_vector.data\n",
    "t_vector[2] =88\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data与原张量的差异"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 属性-dtype\n",
    "\n",
    "- Tensoor元素类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(t_vector.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 属性-grad,grad_fn,requires_grad\n",
    "\n",
    "- 导数：\n",
    "    - 默认是None\n",
    "    - 调用backward计算导数，导数是累加的。如果每次单独计算，需要清空；\n",
    "    - 导数的计算需要导数函数grad_fn（没有指定函数的张量无法计算导数）。\n",
    "    - grad_fn函数自动跟踪，需要设置requires_grad=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. grad属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(t_vector.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-771cce0a66d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "t_vector.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 属性-grad_fn\n",
    "    - 张量所在的导数函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.Tensor([1.0])\n",
    "t2 = torch.Tensor([1.0])\n",
    "\n",
    "t3 = t1 + t2\n",
    "print(t3)\n",
    "print(t3.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. requires_grad属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x10cd1b780>\n",
      "<class 'AddBackward0'>\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "t1.requires_grad=True\n",
    "t2.requires_grad=True\n",
    "t4 = t1 + t2\n",
    "print(t4)\n",
    "print(t4.grad_fn)\n",
    "print(type(t4.grad_fn))\n",
    "print(t4.requires_grad)\n",
    "print(t1.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "tensor([2.])\n",
      "tensor([2.])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(t4.grad)\n",
    "t4.backward()\n",
    "print(t4.grad)    # 没有导数\n",
    "print(t1.grad)     # t1与t2导数（偏导数）\n",
    "print(t2.grad)\n",
    "print(t1.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 属性-is_cuda，device\n",
    "\n",
    "- 判断是否是cuda计算（GPU计算）\n",
    "- device使用专门的类构造；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.Tensor([2.5])\n",
    "print(t1.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.Tensor([2.5], device=torch.device('cpu:0'))\n",
    "t2 = torch.Tensor([2.5], device=torch.device('cpu'))\n",
    "print(t2.is_cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "legacy constructor for device type: cpu was passed device type: cuda, but device type must be: cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8168e2f27255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 苹果电脑不支持，请在Nvidia的显卡上运算，其他支持GPU运算的电脑上运行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: legacy constructor for device type: cpu was passed device type: cuda, but device type must be: cpu"
     ]
    }
   ],
   "source": [
    "t3 = torch.Tensor([2.5], device=torch.device('cuda'))\n",
    "print(t3.is_cuda)   # 苹果电脑不支持，请在Nvidia的显卡上运算，其他支持GPU运算的电脑上运行\n",
    "# 在window上还需要安装厂商驱动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# 判定电脑是否之处GPU运算\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 属性-is_leaf，grad与retain_grad函数\n",
    "\n",
    "- 这个属性用来判定张量Tensor是否是Leaf Tensor，下面两种情况都应该是Leaf Tensor：\n",
    "    - 属性requires_grad为False的。\n",
    "    - 属性requires_grad=True，但是用户构建的Tensor，表示该张量不是计算结果，而是用户构建的初始张量。\n",
    "    \n",
    "- 运行backward后，仅仅只有Leaf Tensor在才会有grad属性。如果非Leaf Tensor需要具有grad属性，需要使用retain_grad函数开启grad属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True False\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# 演示叶子Tensor与grad，backward的关系\n",
    "import torch\n",
    "\n",
    "t1 = torch.Tensor([1.0])     # 用户构建的都是Leaf Tensor\n",
    "t1.requires_grad=True\n",
    "\n",
    "t2 = torch.Tensor([2.0])\n",
    "t2.requires_grad=True\n",
    "\n",
    "t3 = t1 + t2\n",
    "t3.backward()\n",
    "\n",
    "print(t1.is_leaf, t2.is_leaf, t3.is_leaf)\n",
    "print(t1.grad)     # Leaf Tensor的grad属性由backward函数产生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True False\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# 演示Non-Leaf Tensor 与 retain_grad的关系\n",
    "\n",
    "import torch\n",
    "\n",
    "t1 = torch.Tensor([1.0])     # 用户构建的都是Leaf Tensor\n",
    "t1.requires_grad=True\n",
    "\n",
    "t2 = torch.Tensor([2.0])\n",
    "t2.requires_grad=True\n",
    "\n",
    "t3 = t1 + t2\n",
    "t3.retain_grad()    # 调用该函数后，t3才有grad属性，可以注释这个语句体验\n",
    "t3.backward()\n",
    "\n",
    "print(t1.is_leaf, t2.is_leaf, t3.is_leaf)\n",
    "print(t3.grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 属性-ndim与dim函数\n",
    "\n",
    "- Tensor的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.Tensor([1.0, 20])     # 用户构建的都是Leaf Tensor\n",
    "t2 = torch.Tensor(\n",
    "    [\n",
    "        [2.0, 1.0],\n",
    "        [1.0, 2.0]\n",
    "    ]\n",
    ")\n",
    "print(t1.ndim)   # 1 维\n",
    "\n",
    "print(t2.ndim)  # 2 维\n",
    "print(t2.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 属性-shape与size函数\n",
    "\n",
    "- Tensor的形状，与size函数一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.Tensor([1.0, 20])     # 用户构建的都是Leaf Tensor\n",
    "t2 = torch.Tensor(\n",
    "    [\n",
    "        [2.0, 1.0],\n",
    "        [1.0, 2.0]\n",
    "    ]\n",
    ")\n",
    "print(t2.shape) # 属性shape\n",
    "\n",
    "print(t2.size())  # 函数size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 属性-is_sparse\n",
    "\n",
    "- 是否稀疏张量：\n",
    "    - 在矩阵中，若数值为0的元素数目远远多于非0元素的数目，并且非0元素分布没有规律时，则称该矩阵为稀疏矩阵；与之相反，若非0元素数目占大多数时，则称该矩阵为稠密矩阵。定义非零元素的总数比上矩阵所有元素的总数为矩阵的稠密度。\n",
    "    \n",
    "- is_sparse该属性是只读，不可写的\n",
    "    - 稀疏张量提供专门的API产生。\n",
    "    \n",
    "    \n",
    "- 稀疏张量有自己的构造规则：\n",
    "    - 稀疏张量被表示为一对致密张量：一维张量和二维张量的索引。可以通过提供这两个张量来构造稀疏张量，以及稀疏张量的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "attribute 'is_sparse' of 'torch._C._TensorBase' objects is not writable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-180881d46907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mt3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: attribute 'is_sparse' of 'torch._C._TensorBase' objects is not writable"
     ]
    }
   ],
   "source": [
    "# 默认的张量都是稠密张量\n",
    "import torch\n",
    "t1 = torch.Tensor([0, 0])     # 用户构建的都是Leaf Tensor\n",
    "t2 = torch.Tensor(\n",
    "    [\n",
    "        [1, 0],\n",
    "        [0, 0]\n",
    "    ]\n",
    ")\n",
    "print(t1.is_sparse)  # 属性shape\n",
    "\n",
    "print(t2.is_sparse)  # 函数size()\n",
    "\n",
    "t3 = torch.Tensor(1000,1000)\n",
    "t3.fill_(0)\n",
    "t3[0,0]=1\n",
    "print(t3.is_sparse)  \n",
    "print(t3)\n",
    "t3.is_sparse=True   # 不能修改该属性，该属性是只读，不可写的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor(indices=tensor([], size=(2, 0)),\n",
      "       values=tensor([], size=(0,)),\n",
      "       size=(2, 3), nnz=0, layout=torch.sparse_coo)\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 稀疏矩阵\n",
    "import torch\n",
    "ts = torch.sparse.FloatTensor(2, 3)\n",
    "print(ts.is_sparse)  \n",
    "print(ts)\n",
    "print(ts.to_dense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 属性-layout\n",
    "\n",
    "- 张量Tensor使用Storage表示都是一维的，其构成张量只要采用布局计算。这个布局使用layout属性设置\n",
    "    - 一般都是采用strided\n",
    "    - 稀疏矩阵的布局使用的是：`torch.sparse_coo`\n",
    "    \n",
    "- 目前常用的就是这两种布局layout。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.strided torch.strided\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.Tensor([0, 0])     # 用户构建的都是Leaf Tensor\n",
    "t2 = torch.Tensor(\n",
    "    [\n",
    "        [1, 0],\n",
    "        [0, 0]\n",
    "    ]\n",
    ")\n",
    "print(t1.layout, t2.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 属性-output_nr\n",
    "\n",
    "- 在反向传播中存放输出。\n",
    "    - 具体用途先存疑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "t1 = torch.Tensor([0, 0])     # 用户构建的都是Leaf Tensor\n",
    "t2 = torch.Tensor(\n",
    "    [\n",
    "        [1, 0],\n",
    "        [0, 0]\n",
    "    ]\n",
    ")\n",
    "print(t2.output_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n",
      "tensor([0.9093], grad_fn=<SinBackward>)\n",
      "True True False\n",
      "tensor([1.])\n",
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "# 演示Non-Leaf Tensor 与 retain_grad的关系\n",
    "\n",
    "import torch\n",
    "\n",
    "t1 = torch.Tensor([2.0])     # 用户构建的都是Leaf Tensor\n",
    "t1.requires_grad=True\n",
    "\n",
    "t2 = torch.Tensor([2.0])\n",
    "t2.requires_grad=True\n",
    "\n",
    "t3 = t1.sin()\n",
    "print(t1.output_nr, t2.output_nr, t2.output_nr)\n",
    "t3.retain_grad()    # 调用该函数后，t3才有grad属性，可以注释这个语句体验\n",
    "t3.backward()\n",
    "print(t3)\n",
    "print(t1.is_leaf, t2.is_leaf, t3.is_leaf)\n",
    "print(t3.grad) \n",
    "print(t1.output_nr, t2.output_nr, t2.output_nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 属性-其他\n",
    "\n",
    "- is_mkldnn：intel提供的加速CPU运算的方法，判定是否CPU加速\n",
    "- is_quantized：是否被量化（量化指将信号的连续取值近似为有限多个离散值）\n",
    "- name：张量名\n",
    "- volatile：新版本已经停用；\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 演示Non-Leaf Tensor 与 retain_grad的关系\n",
    "\n",
    "import torch\n",
    "\n",
    "t1 = torch.Tensor([2.0])     # 用户构建的都是Leaf Tensor\n",
    "t1.requires_grad=True\n",
    "\n",
    "t2 = torch.Tensor([2.0])\n",
    "t2.requires_grad=True\n",
    "\n",
    "t3 = t1 + t2\n",
    "print(t1.is_mkldnn)\n",
    "print(t2.name)\n",
    "print(t2.is_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附录：mkldnn的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 下载地址\n",
    "    - `https://github.com/intel/mkl-dnn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 安装\n",
    "    - cmake安装，直接套路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 如果torch不支持mkldnn，就需要使用源代码重新安装！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
