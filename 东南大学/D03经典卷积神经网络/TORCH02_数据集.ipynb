{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch提供了很多数据集\n",
    "    - MNIST\n",
    "    - Fashion-MNIST\n",
    "    - KMNIST\n",
    "    - EMNIST\n",
    "    - QMNIST\n",
    "    - FakeData\n",
    "    - COCO\n",
    "    - LSUN\n",
    "    - ImageFolder\n",
    "    - DatasetFolder\n",
    "    - ImageNet\n",
    "    - CIFAR\n",
    "    - STL10\n",
    "    - SVHN\n",
    "    - PhotoTour\n",
    "    - SBU\n",
    "    - Flickr\n",
    "    - VOC\n",
    "    - Cityscapes\n",
    "    - SBD\n",
    "    - USPS\n",
    "    - Kinetics-400\n",
    "    - HMDB51\n",
    "    - UCF101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST类说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "CLASS torchvision.datasets.MNIST(\n",
    "    root,                                # 数据集的存放目录\n",
    "    train=True,                          # True表示加载Train数据集，否则加载Test数据集\n",
    "    transform=None,                      # 变换函数，用来对数据特征进行变换处理\n",
    "    target_transform=None,               # 变换函数，用来对数据特征进行变换处理\n",
    "    download=False)                      # True表示从互联网下载数据集到root，否则认为已经下载，直接从root读取(下载的时候，下载全部数据集)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n",
      "<class 'torchvision.datasets.mnist.MNIST'>\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "ds_mnist = MNIST(root=\"./datasets\", download=True)\n",
    "print(type(ds_mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST类使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 所有的数据集类型都是DataSet类型\n",
    "    - `torch.utils.data.Dataset`\n",
    "    - 数据集分成两类：\n",
    "        - map-style datasets,\n",
    "            - torch.utils.data.Dataset\n",
    "        - iterable-style datasets.\n",
    "            - torch.utils.data.IterableDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MNIST属于map-style datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是Map数据集\n",
      "不是迭代数据集\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "ds_mnist = MNIST(root=\"./datasets\", download=False)\n",
    "if isinstance(ds_mnist, Dataset) :\n",
    "    print(\"是Map数据集\")\n",
    "\n",
    "if isinstance(ds_mnist, IterableDataset) :   \n",
    "    print(\"是Iterable数据集\")\n",
    "else:\n",
    "    print(\"不是迭代数据集\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. MNIST的集成结构是：\n",
    "    - MNIST\n",
    "       - |   -torchvision.datasets.vision.VisionDataset\n",
    "       - |   -torch.utils.data.dataset.Dataset\n",
    "       - |   -builtins.object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. MNIST是Map-Style的数据集\n",
    "    - 数据遍历方式：\n",
    "        1. 取长度：`__len__(self)`\n",
    "        2. 根据下标获取元素：`__getitem__(self, index)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "<class 'tuple'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c2950b82e8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAOL0lEQVR4nO3dfYhd9Z3H8c+nukkxGw2T7iYSo5JYY43KLMaHVcEVDSFF/xijXUP9Y+tDWGtoWEpA7LJgISL4wBqMdAK7UlfWal2CT7BKSKJ/KJGpxnYdiE3rU9xJ6aKJMTHaZL77x1zdu+Pc353c58z3/YLA3PM9554vBz+ec+/vnPtzRAjA1PaNbjcAoP0IOpAAQQcSIOhAAgQdSICgAwkc36kd2WYcD2iziPBEy5s6o9v+nu13bO+yfVMz7wWgfdzoDTO2Z0oalnSxpCOSdkg6NyL+WGN9zuhAm7XjjL5M0ksR8WFE7JG0RdKVTbwfgDZp5jP6fEnvVb3eLenk6hVsr5K0qol9AGiBZoI+TdJo1etRjV3CfyUiNkraKHHpDnRTM5fuI5LmVb0+RdIHzbUDoB2a+TJujqTXJf2Vxv6H8YrGvow7UGN9zuhAm9X6Mq7hS/eI+IPtn0h6tbLox7VCDqC7Gj6jH/WOOKMDbdeWG2YAHBsIOpAAQQcSIOhAAgQdSICgAwkQdCABgg4kQNCBBAg6kABBBxIg6EACBB1IgKADCRB0IAGCDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQQIOpAAQQcSIOhAAgQdSKDhaZPR+4477rhi/aSTTmrr/levXl2zdsIJJxS3XbRoUbF+++23F+v33XdfzdrKlSuL2x46dKhYv+eee4r1u+66q1jvhqaCbvstSdMrL1+OiJuabwlAqzV7Rp8eEWe0pBMAbdPsZ/RoSRcA2qrhoNueIWmO7d/b3mr7ggnWWWV7yPZQU10CaErDl+4RcUDSiZJk+3pJmySdMm6djZI2Vtbh7A90SUuG1yLil5K+aXtWK94PQGs1c+l+ku3Zlb+XS/ooIva2rDMALdPMt+59kjbblqQ9kq5vSUdTzKmnnlqsT5s2rVi/5JJLivXLLrusZm3WrPIF1ooVK4r1btq9e3exvn79+mJ9YGCgZm3//v3Fbd98881i/aWXXirWe1Ezn9HfkbSwhb0AaBNugQUSIOhAAgQdSICgAwkQdCABR3TmhrWpemdcf39/sb5ly5Zivd2Pivaq0dHRYv2mm8oPQn766acN73tkZKRY//jjj4v1nTt3NrzvdosIT7ScMzqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJMA4epP6+vqK9e3btxfrCxYsaGU7LVWv9717yz8/cMUVV9SsffHFF8Vts95f0CzG0YHECDqQAEEHEiDoQAIEHUiAoAMJEHQgAaZNbtJHH31UrK9du7ZYv/rqq4v1N954o1iv97PHJTt27CjWly5dWqwfOHCgWF+8eHHN2po1a4rborU4owMJEHQgAYIOJEDQgQQIOpAAQQcSIOhAAjyP3mUnnnhisV5vit/BwcGatZtvvrm47Y033lisP/7448U6ek/Dz6Pbnm77Ntubxi1fY/t92zttL29VowBabzJ3xu2U9IakmV8usL1Q0u2SFkuaL2mz7dMi4k9t6RJAUybzGb1f0oPjlg1IejIi9kfEsKR3JZ3f4t4AtEjdoEfERD8MNl/Se1Wvd0s6efxKtlfZHrI91HiLAJrV6EMt0yRVz5I3KunI+JUiYqOkjRJfxgHd1Ojw2oikeVWvT5H0QfPtAGiHRoP+vKQbbJ9g+zuS+iSVn3kE0DUNXbpHxK9sPybpLUmHJN0SnRqQn2I++eSTprbft29fw9veeuutxfoTTzxRrNeb4xy9Y1JBj4htkraNW3a3pLtb3xKAVuMWWCABgg4kQNCBBAg6kABBBxLgMdVj3IwZM2rWnn322eK2l19+ebG+fHn5ocQXX3yxWEfnMW0ykBhBBxIg6EACBB1IgKADCRB0IAGCDiTAOPoUtnDhwmL99ddfL9b37p3oV8T+z9atW4v1oaHavyC2YcOG4rY89dwYxtGBxAg6kABBBxIg6EACBB1IgKADCRB0IAHG0RMbGBgo1h955JFifebMmcV6yZ133lmsP/roo8X6yMhIw/ueyhhHBxIj6EACBB1IgKADCRB0IAGCDiRA0IEEGEdHTeecc06x/sADDxTrV155ZcP7HhwcLNbXrVtXrH/44YcN7/tY1vA4uu3ptm+zvWnc8n22d1X+/bRVjQJovcnMj75T0huSvroNyvZ0Se9HxLntagxA60zmM3q/pAfHLZst6ePWtwOgHeoGPSIm+uGwWZIW2/6d7edsnzHRtrZX2R6yXfvHwwC0XUPfukfEcETMlvRtSVsl/bzGehsjYklELGmiRwBNamp4LSJGJQ1KWtyadgC0Q0NBtz3H9pfz9d4o6bXWtQSg1SY1jm77byT9Y0RcVXn915J+IemwpF2SVkXEe3Xeg3H0KWbWrFnF+jXXXFOzVu9Zd3vC4eCvbNmypVhfunRpsT5V1RpHn8zwmiJim6RtVa9flXRaKxoD0H7cAgskQNCBBAg6kABBBxIg6EACPKaKrvj888+L9eOPLw8IHT58uFhftmxZzdq2bduK2x7L+LlnIDGCDiRA0IEECDqQAEEHEiDoQAIEHUhgUk+vIafzzjuvWL/uuuuK9QsuuKBmrd44eT3Dw8PF+ssvv9zU+081nNGBBAg6kABBBxIg6EACBB1IgKADCRB0IAHG0aewRYsWFeurV68u1q+99tpife7cuUfd02QdOXKkWB8ZGSnWR0dHW9nOMY8zOpAAQQcSIOhAAgQdSICgAwkQdCABgg4kwDh6j6s3Vr1y5cqatXrj5KeffnojLbXE0NBQsb5u3bpi/ZlnnmllO1Ne3TO67Wm2H7b9tu3f2l5RWb7G9vu2d9pe3v5WATRqMmf0PklbIuKHts+U9Jrt/5J0u6TFkuZL2mz7tIj4Uxt7BdCgumf0iNgTEU9V/n5b0mFJN0h6MiL2R8SwpHclnd/ORgE07qi+jLP9A0m/1thZ/r2q0m5JJ0+w/irbQ7bLH8gAtNWkg277Dkk/kvR9SdMkVT81MCrpa08hRMTGiFgSEUuabRRA4yb1rbvtDZJmSLo0Ig7aHpE0r2qVUyR90Ib+ALRA3aDbvljSooi4qmrx85L+zfZ9kk7T2KX8jva0eGybM2dOsX722WcX6w899FCxftZZZx11T62yffv2Yv3ee++tWXv66aeL2/KYaWtN5ozeL2mJ7V1Vy1ZLekzSW5IOSbolOjXROoCjVjfoEfEzST+boPSfku5ueUcAWo5bYIEECDqQAEEHEiDoQAIEHUiAx1Qnoa+vr2ZtcHCwuG1/f3+xvmDBgoZ6aoVXXnmlWL///vuL9RdeeKFY/+yzz466J7QHZ3QgAYIOJEDQgQQIOpAAQQcSIOhAAgQdSCDFOPpFF11UrK9du7ZYv/DCC2vW5s2bV7PWCQcPHqxZW79+fXHbu+8uP3x44MCBhnpC7+GMDiRA0IEECDqQAEEHEiDoQAIEHUiAoAMJpBhHHxgYaKrejOHh4WL9ueeeK9YPHz5crJeeGd+7d29xW+TBGR1IgKADCRB0IAGCDiRA0IEECDqQAEEHEnC92Y5tT5P0z5KukmRJd0TEf9jeJ+mPldX+PSL+qc77MK0y0GYR4YmWTybocyVdFhFP2T5T0muS5kgaiohzJ9sAQQfar1bQ6166R8SeiHiq8vfbkg5Lmivp45Z2CKBtjuozuu0fSPq1pBmSFtv+ne3nbJ9RY/1VtodsD7WgVwANqnvp/tWK9h2S/lbSdyNipLLsG5L+QdK1EXFpne25dAfarOHP6JJke4PGzuI/jIiD42p/Lml3RMyq8x4EHWizWkGv+/Sa7YslLYqIq6qWzZH0aUQckHSjxr6gA9CjJvOYar+kJbZ3VS37F0l/b/uwpF2Sbm1HcwBaY9Kf0ZveEZfuQNs1PLwG4NhH0IEECDqQAEEHEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQQIOpAAQQcSIOhAAgQdSKCT0yb/j6T3ql5/q7KsF/Vqb73al0RvjWplb6fVKnTsefSv7dgeioglXdl5Hb3aW6/2JdFbozrVG5fuQAIEHUigm0Hf2MV919OrvfVqXxK9NaojvXXtMzqAzuHSHUiAoAMJdDzotr9n+x3bu2zf1On912P7rUpvu2z/a5d7mW77Ntubxi1fY/t92zttL++hvvZVHbufdrqvSg/TbD9s+23bv7W9orK8q8esTm/tP24R0bF/kmZK+kDSPI3NyLpH0l90sodJ9Lir2z1U9fKupE2SNlctWyjp7cqxPFvSf0v6sx7oa7qk3/TAMZsr6brK32dK2itpUbePWaG3jhy3Tp/Rl0l6KSI+jIg9krZIurLDPdTTS99O9kt6cNyyAUlPRsT+iBjWWOjO74G+ZqsHptKOiaf5vkHdP2a1euvIFOSdDvp8/f/bYHdLOrnDPdRke4akObZ/b3ur7Qu62U9E7J1gcdePYY2+ZmkSU2l3UtU0333qsf/ujnYK8mZ1OujTJI1WvR6VdKTDPdQUEQci4sSIWCDpYY1dnvaanjyGETEcEbMlfVvSVkk/72Y/lWm+fyTp++qxY1bdW6eOW6eDPqKxz+dfOkVjn9l7TkT8UtI3bReng+6Cnj6GETEqaVDS4m71UJnm+yxJl0bEiHromE3Qm6T2H7dOB/0FScts/6XtuZIukfRih3uoyfZJtmdX/l4u6aMal6nd9LykG2yfYPs7Grss3dHlnmR7TuWjj9TFqbSrpvn+u4g4WFncE8dsot46ddw6+ZiqIuIPtn8i6dXKoh/H2BzrvaJP0mbb0tiIwPXdbefrIuJXth+T9JakQ5JuicrXuF22QNIvemAq7Ymm+V4tqReOWdemIOcWWCAB7owDEiDoQAIEHUiAoAMJEHQgAYIOJEDQgQQIOpAAQQcS+F9u039hoY1iIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST\n",
    "train_mnist = MNIST(root=\"./datasets\", train=True, download=False)\n",
    "print(len(train_mnist))\n",
    "test_mnist = MNIST(root=\"./datasets\", train=False, download=False)\n",
    "print(len(test_mnist))\n",
    "print(type(train_mnist[0]))\n",
    "plt.imshow(train_mnist[0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 数据的属性:\n",
    "    - 与数据有关的属性\n",
    "        1. class_to_idx\n",
    "        2. processed_folder\n",
    "        3. raw_folder\n",
    "        4. test_data\n",
    "        5. test_labels\n",
    "        6. train_data\n",
    "        7. train_labels\n",
    "    - 与资源有关的属性\n",
    "        1. classes = `['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', ...`\n",
    "        2. resources = `[('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyt...`\n",
    "        3. test_file = 'test.pt'\n",
    "        4. training_file = 'training.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0 - zero': 0, '1 - one': 1, '2 - two': 2, '3 - three': 3, '4 - four': 4, '5 - five': 5, '6 - six': 6, '7 - seven': 7, '8 - eight': 8, '9 - nine': 9}\n",
      "./datasets\\MNIST\\processed\n",
      "./datasets\\MNIST\\raw\n",
      "torch.Size([10000, 28, 28])\n",
      "torch.Size([10000])\n",
      "torch.Size([10000, 28, 28])\n",
      "torch.Size([10000])\n",
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
      "[('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', 'f68b3c2dcbeaaa9fbdd348bbdeb94873'), ('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', 'd53e105ee54ea40749a09fcbcd1e9432'), ('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', '9fb629c4189551a2d022fa330f9573f3'), ('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', 'ec29112dd5afa0611ce80d1b7f02629c')]\n",
      "test.pt\n",
      "training.pt\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "test_mnist = MNIST(root=\"./datasets\", train=False, download=False)\n",
    "print(test_mnist.class_to_idx)\n",
    "print(test_mnist.processed_folder)\n",
    "print(test_mnist.raw_folder)\n",
    "print(test_mnist.test_data.shape)\n",
    "print(test_mnist.test_labels.shape)\n",
    "print(test_mnist.train_data.shape)\n",
    "print(test_mnist.train_labels.shape)\n",
    "print(test_mnist.classes)\n",
    "print(test_mnist.resources)\n",
    "print(test_mnist.test_file)\n",
    "print(test_mnist.training_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MNIST in module torchvision.datasets.mnist object:\n",
      "\n",
      "class MNIST(torchvision.datasets.vision.VisionDataset)\n",
      " |  `MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
      " |  \n",
      " |  Args:\n",
      " |      root (string): Root directory of dataset where ``MNIST/processed/training.pt``\n",
      " |          and  ``MNIST/processed/test.pt`` exist.\n",
      " |      train (bool, optional): If True, creates dataset from ``training.pt``,\n",
      " |          otherwise from ``test.pt``.\n",
      " |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      " |          puts it in root directory. If dataset is already downloaded, it is not\n",
      " |          downloaded again.\n",
      " |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      " |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      " |      target_transform (callable, optional): A function/transform that takes in the\n",
      " |          target and transforms it.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MNIST\n",
      " |      torchvision.datasets.vision.VisionDataset\n",
      " |      torch.utils.data.dataset.Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Args:\n",
      " |          index (int): Index\n",
      " |      \n",
      " |      Returns:\n",
      " |          tuple: (image, target) where target is index of the target class.\n",
      " |  \n",
      " |  __init__(self, root, train=True, transform=None, target_transform=None, download=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  download(self)\n",
      " |      Download the MNIST data if it doesn't exist in processed_folder already.\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  class_to_idx\n",
      " |  \n",
      " |  processed_folder\n",
      " |  \n",
      " |  raw_folder\n",
      " |  \n",
      " |  test_data\n",
      " |  \n",
      " |  test_labels\n",
      " |  \n",
      " |  train_data\n",
      " |  \n",
      " |  train_labels\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', ...\n",
      " |  \n",
      " |  resources = [('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyt...\n",
      " |  \n",
      " |  test_file = 'test.pt'\n",
      " |  \n",
      " |  training_file = 'training.pt'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ds_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map样式-Dataset类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset抽象类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 支持`__add__操作`:`+`\n",
    "- 支持`__getitem__`操作：`[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dataset in module torch.utils.data.dataset:\n",
      "\n",
      "class Dataset(builtins.object)\n",
      " |  An abstract class representing a :class:`Dataset`.\n",
      " |  \n",
      " |  All datasets that represent a map from keys to data samples should subclass\n",
      " |  it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
      " |  data sample for a given key. Subclasses could also optionally overwrite\n",
      " |  :meth:`__len__`, which is expected to return the size of the dataset by many\n",
      " |  :class:`~torch.utils.data.Sampler` implementations and the default options\n",
      " |  of :class:`~torch.utils.data.DataLoader`.\n",
      " |  \n",
      " |  .. note::\n",
      " |    :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
      " |    sampler that yields integral indices.  To make it work with a map-style\n",
      " |    dataset with non-integral indices/keys, a custom sampler must be provided.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "help(Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `+`操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据集的合并操作\n",
    "    - 支持`__add__`运算`+`\n",
    "    - 返回的类型是：ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.ConcatDataset'> 70000\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "train_mnist = MNIST(root=\"./datasets\", train=True, download=False)\n",
    "test_mnist = MNIST(root=\"./datasets\", train=False, download=False)\n",
    "data = train_mnist + test_mnist\n",
    "print(type(data), len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConcatDataset类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 提供使用列表形式提供数据合并。（构造模式）\n",
    "- 提供静态函数：cumsum(sequence)（工厂模式）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 使用序列合并数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.ConcatDataset'> 70000\n",
      "Help on ConcatDataset in module torch.utils.data.dataset object:\n",
      "\n",
      "class ConcatDataset(Dataset)\n",
      " |  Dataset as a concatenation of multiple datasets.\n",
      " |  \n",
      " |  This class is useful to assemble different existing datasets.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      datasets (sequence): List of datasets to be concatenated\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ConcatDataset\n",
      " |      Dataset\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, idx)\n",
      " |  \n",
      " |  __init__(self, datasets)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  cumsum(sequence)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  cummulative_sizes\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Dataset:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Dataset:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "train_mnist = MNIST(root=\"./datasets\", train=True, download=False)\n",
    "test_mnist = MNIST(root=\"./datasets\", train=False, download=False)\n",
    "data = train_mnist + test_mnist\n",
    "\n",
    "# 可是使用构造器构造\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "all_data = ConcatDataset([train_mnist, test_mnist])\n",
    "print(type(all_data), len(all_data))\n",
    "\n",
    "help(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. cumsum函数与cumulative_sizes属性\n",
    "    - cummulative_sizes名字已经改成cumulative_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60000, 70000]\n",
      "[60000, 70000]\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "\n",
    "train_mnist = MNIST(root=\"./datasets\", train=True, download=False)\n",
    "test_mnist = MNIST(root=\"./datasets\", train=False, download=False)\n",
    "all_data = ConcatDataset([train_mnist, test_mnist])\n",
    "print(all_data.cumulative_sizes)\n",
    "# ----------------------------------\n",
    "list_sum = ConcatDataset.cumsum([train_mnist , test_mnist])\n",
    "print(list_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterable样式-IterableDataset类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 提供`__iter__(self)`实现的类。\n",
    "- 提供`__add__(self, other)`实现数据添加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这个类是规范类，用户集成用来实现自己的可迭代数据集。\n",
    "    - 这个类的规范在于可以被DataLoader访问使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "<enumerate object at 0x000001C28D518558>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "class RangeDataset(IterableDataset):\n",
    "    def __init__(self, start, end):\n",
    "        self.start=start\n",
    "        self.end=end\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.end))\n",
    "\n",
    "ds_range = RangeDataset(1,5)\n",
    "lst = [x for x in ds_range]\n",
    "print(lst)\n",
    "\n",
    "print(list(ds_range))\n",
    "print(enumerate(ds_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader与Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader 类说明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataLoader作用\n",
    "    - 主要用来对数据集进行分配，DataLoader的工作是基于Dataset与IterableDataset的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "CLASStorch.utils.data.DataLoader(\n",
    "    dataset,                         # 需要分匹配的数据集\n",
    "    batch_size=1,                    # 范培的批次大小\n",
    "    shuffle=False,                   # 是否洗牌\n",
    "    sampler=None,                    # 设置采样器\n",
    "    batch_sampler=None,              # 批次采样器\n",
    "    num_workers=0,                   # 数据处理的子进程任务数\n",
    "    collate_fn=None,                 # 合并子进程数据的协作函数\n",
    "    pin_memory=False,                # 拷贝数据到CUDA\n",
    "    drop_last=False,                 # 是否丢弃不足批次数的\n",
    "    timeout=0,                       # 设置加载时间（0：加载到完成为止）\n",
    "    worker_init_fn=None,             # 一个回调函数，在worker数据前后后调用。\n",
    "    multiprocessing_context=None)    # 进程上下文，一般情况设置为None，表示使用默认当前进程的上下文。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataLoader in module torch.utils.data.dataloader:\n",
      "\n",
      "class DataLoader(builtins.object)\n",
      " |  Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
      " |  the given dataset.\n",
      " |  \n",
      " |  The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
      " |  iterable-style datasets with single- or multi-process loading, customizing\n",
      " |  loading order and optional automatic batching (collation) and memory pinning.\n",
      " |  \n",
      " |  See :py:mod:`torch.utils.data` documentation page for more details.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      dataset (Dataset): dataset from which to load the data.\n",
      " |      batch_size (int, optional): how many samples per batch to load\n",
      " |          (default: ``1``).\n",
      " |      shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
      " |          at every epoch (default: ``False``).\n",
      " |      sampler (Sampler, optional): defines the strategy to draw samples from\n",
      " |          the dataset. If specified, :attr:`shuffle` must be ``False``.\n",
      " |      batch_sampler (Sampler, optional): like :attr:`sampler`, but returns a batch of\n",
      " |          indices at a time. Mutually exclusive with :attr:`batch_size`,\n",
      " |          :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.\n",
      " |      num_workers (int, optional): how many subprocesses to use for data\n",
      " |          loading. ``0`` means that the data will be loaded in the main process.\n",
      " |          (default: ``0``)\n",
      " |      collate_fn (callable, optional): merges a list of samples to form a\n",
      " |          mini-batch of Tensor(s).  Used when using batched loading from a\n",
      " |          map-style dataset.\n",
      " |      pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
      " |          into CUDA pinned memory before returning them.  If your data elements\n",
      " |          are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
      " |          see the example below.\n",
      " |      drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
      " |          if the dataset size is not divisible by the batch size. If ``False`` and\n",
      " |          the size of dataset is not divisible by the batch size, then the last batch\n",
      " |          will be smaller. (default: ``False``)\n",
      " |      timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
      " |          from workers. Should always be non-negative. (default: ``0``)\n",
      " |      worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
      " |          worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
      " |          input, after seeding and before data loading. (default: ``None``)\n",
      " |  \n",
      " |  \n",
      " |  .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
      " |               cannot be an unpicklable object, e.g., a lambda function. See\n",
      " |               :ref:`multiprocessing-best-practices` on more details related\n",
      " |               to multiprocessing in PyTorch.\n",
      " |  \n",
      " |  .. note:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
      " |            When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
      " |            ``len(dataset)`` (if implemented) is returned instead, regardless\n",
      " |            of multi-process loading configurations, because PyTorch trust\n",
      " |            user :attr:`dataset` code in correctly handling multi-process\n",
      " |            loading to avoid duplicate data. See `Dataset Types`_ for more\n",
      " |            details on these two types of datasets and how\n",
      " |            :class:`~torch.utils.data.IterableDataset` interacts with `Multi-process data loading`_.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __setattr__(self, attr, val)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  multiprocessing_context\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "help(DataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常规使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 常规使用主要是如下几个常用的参数：\n",
    "    1. dataset必须的\n",
    "    2. batch_size批次数据个数\n",
    "    3. shuffle是否洗牌打乱（随机洗牌）\n",
    "    4. drop_last是否丢弃剩余不足一个批次的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用模式：\n",
    "    - 迭代获取数据，获取的数据是Tensor类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 例子1：迭代数据集\n",
    "    - 迭代数据集只有一个循环元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([2])\n",
      "tensor([3])\n",
      "tensor([4])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class RangeDataset(IterableDataset):\n",
    "    def __init__(self, start, end):\n",
    "        self.start=start\n",
    "        self.end=end\n",
    "    def __iter__(self):\n",
    "        return iter(range(self.start, self.end))\n",
    "\n",
    "ds_range = RangeDataset(1,5)\n",
    "\n",
    "loader = DataLoader(ds_range)\n",
    "for item in loader:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 例子2：map数据集\n",
    "    - 循环的是一个大小为2的元祖。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 注意：\n",
    "    - MNIST的数据加载时PIL.Image.Image类型，需要使用函数转换下，这里先使用把Image转换为Tensor的函数ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n",
      "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n",
      "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n",
      "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n",
      "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n",
      "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "train_mnist = MNIST(root=\"./datasets\", train=True, download=False,transform=ToTensor())\n",
    "\n",
    "loader = DataLoader(train_mnist, batch_size=10000, shuffle=True, drop_last=False)\n",
    "for d, t in loader:    # 数据与标签\n",
    "    print(d.shape, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 采样器的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch提供了抽象类Sampler实现数据采样，实现用户定制的迭代返回数据。\n",
    "    - 该类提供两个接口函数，实现数据处理的规范：\n",
    "        1. `__iter__()`\n",
    "            - 返回数据集索引的迭代器。\n",
    "        2. `__len__()`\n",
    "            - 返回迭代器的个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据采样器参数sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 定义一个采样器\n",
    "    - sampler采样器返回的必须是整数迭代器。\n",
    "    - `__len__`在这里没有作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "class MySampler(Sampler):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __iter__(self):\n",
    "        return iter(range(0,4))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用采样器\n",
    "    - 采样器sampler参数与shuffle参数矛盾的。\n",
    "    - smapler负责从原数据集获取数据。（默认是全部采样）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28]) torch.Size([4])\n",
      "-------------------------------\n",
      "torch.Size([2, 1, 28, 28]) torch.Size([2])\n",
      "torch.Size([2, 1, 28, 28]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_mnist = MNIST(root=\"./datasets\", train=False, download=False)\n",
    "\n",
    "sampler = MySampler()\n",
    "loader = DataLoader(train_mnist, sampler=sampler, batch_size=100, drop_last=False)\n",
    "for d, t in loader:\n",
    "    print(d.shape, t.shape)\n",
    "print(\"-------------------------------\")\n",
    "loader = DataLoader(train_mnist, sampler=sampler, batch_size=2, drop_last=False)\n",
    "for d, t in loader:\n",
    "    print(d.shape, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批次采样器batch_sampler参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 定义批次采样器\n",
    "    - 返回的迭代器的迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "\n",
    "class MyBatchSampler(Sampler):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __iter__(self):\n",
    "        return iter([iter(range(0,4)), iter(range(4,10))])  # 使用生成器也行。\n",
    "    \n",
    "    def __len__(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 使用批次采样器\n",
    "    - 批次采样器参数与下面的参数矛盾，不能同时使用。\n",
    "    - batch_size, shuffle, sampler, and drop_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 28, 28]) torch.Size([4])\n",
      "torch.Size([6, 1, 28, 28]) torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_mnist = MNIST(root=\"./datasets\", train=False, download=False)\n",
    "\n",
    "sampler = MyBatchSampler()\n",
    "loader = DataLoader(train_mnist, batch_sampler=sampler)\n",
    "for d, t in loader:\n",
    "    print(d.shape, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他采样器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch为了规范采样器，提供了分类接口\n",
    "    1. SequentialSampler：序列采样\n",
    "    2. RandomSampler：随机采样\n",
    "    3. SubsetRandomSampler：子集随机采样\n",
    "    4. WeightedRandomSampler：权重随机采样\n",
    "    5. BatchSampler：批次采样\n",
    "    6. DistributedSampler：分布式采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import SequentialSampler\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import DistributedSampler\n",
    "\n",
    "test_mnist = MNIST(root=\"./datasets\", train=False, download=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SequentialSampler采样器的例子\n",
    "    - 注意构造器需要一个数据集，这个数据集主要取他长度形成顺序采样器。顺序是从0，1，2，....开始的，长度与数据集一样。\n",
    "    - 构造器：\n",
    "        - `__init__(self, data_source)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "torch.Size([3, 1, 28, 28]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "ds = [1,3,10]\n",
    "s_sampler = SequentialSampler(ds)   # 使用ds的len作为访问顺序\n",
    "for x in s_sampler:\n",
    "    print(x)\n",
    "print(\"------------------------\")\n",
    "loader = DataLoader(train_mnist, sampler=s_sampler, batch_size=100, drop_last=False)   # 采样前3个。不是采样1，3，10位置上的数据哈\n",
    "for d, t in loader:\n",
    "    print(d.shape, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 实现代码：\n",
    "\n",
    "```python\n",
    "    def __iter__(self):\n",
    "        return iter(range(len(self.data_source)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. RandomSampler\n",
    "    - 构造器：`__init__(self, data_source, replacement=False, num_samples=None)`\n",
    "        - data_source：数据集；这样产生的随机数不会查过数据集的个数值。\n",
    "        - replacement=True：使用num_samples作为采样个数，false使用数据集长度作为采样个数\n",
    "        - num_samples在replacement=True使用。replacement=False该参数没有意义,还会抛出异常。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 实现代码：\n",
    "\n",
    "```python\n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        # dataset size might change at runtime\n",
    "        if self._num_samples is None:\n",
    "            return len(self.data_source)\n",
    "        return self._num_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = len(self.data_source)\n",
    "        if self.replacement:\n",
    "            return iter(torch.randint(high=n, size=(self.num_samples,), dtype=torch.int64).tolist())\n",
    "        return iter(torch.randperm(n).tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用代码\n",
    "    - 返回的随机序列值不会超过数据集的长度值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "1\n",
      "------------------------\n",
      "torch.Size([3, 1, 28, 28]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "ds = [1,3,10]    # 下面产生的随机数，在[ 0，len（ds）)范围内\n",
    "r_sampler = RandomSampler(ds, replacement=True, num_samples=10)   # replacement=True的使用\n",
    "r_sampler = RandomSampler(ds, replacement=False)   # 使用ds的len作为访问顺序\n",
    "for x in r_sampler:\n",
    "    print(x)\n",
    "print(\"------------------------\")    \n",
    "loader = DataLoader(train_mnist, sampler=r_sampler, batch_size=100, drop_last=False)   # 采样前3个。不是采样1，3，10位置上的数据哈\n",
    "for d, t in loader:\n",
    "    print(d.shape, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. SubsetRandomSampler采样器\n",
    "    - indices参数：一个索引序列。（会被打乱） "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 源代码：\n",
    "    - 直接手工指定子集的索引，并随机排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "\n",
    "class SubsetRandomSampler(Sampler):\n",
    "    r\"\"\"Samples elements randomly from a given list of indices, without replacement.\n",
    "\n",
    "    Arguments:\n",
    "        indices (sequence): a sequence of indices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.randperm(len(self.indices)))    # 对下表随机排序，randperm产生随机全排列\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "5000\n",
      "10\n",
      "1\n",
      "------------------------\n",
      "torch.Size([4, 1, 28, 28]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "ds = [1,3,10, 5000]    \n",
    "sr_sampler = SubsetRandomSampler(ds)  # 采样下表为1，3，10的数据，并打乱返回\n",
    "for x in sr_sampler:\n",
    "    print(x)\n",
    "print(\"------------------------\")    \n",
    "loader = DataLoader(train_mnist, sampler=sr_sampler, batch_size=100, drop_last=False)  \n",
    "for d, t in loader:\n",
    "    print(d.shape, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. WeightedRandomSampler\n",
    "    - weights (sequence)  : 权重，其和可以不为1。\n",
    "    - num_samples (int)   ：样本总数\n",
    "    - replacement (bool): ：放回抽样\n",
    "True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "4\n",
      "1\n",
      "3\n",
      "3\n",
      "4\n",
      "1\n",
      "4\n",
      "1\n",
      "------------------------\n",
      "torch.Size([10, 1, 28, 28]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "w = [0.4, 0.8, 0.2, 0.6, 0.9]    # 权重高的其对应的下表抽取的机会高。 注意：抽的下标，权重知识表示抽到的概率\n",
    "wr_sampler = WeightedRandomSampler(w, 10, True)    # 抽取10个：当最后一个参数为False，则第二个参数要小于等于权重长度\n",
    "for x in wr_sampler:\n",
    "    print(x)\n",
    "print(\"------------------------\")    \n",
    "loader = DataLoader(train_mnist, sampler=wr_sampler, batch_size=100, drop_last=False)  \n",
    "for d, t in loader:\n",
    "    print(d.shape, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不放回抽样\n",
    "    - 注意：抽取的个数必须与权重个数一样或者小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0\n",
      "4\n",
      "1\n",
      "------------------------\n",
      "torch.Size([4, 1, 28, 28]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "w = [0.4, 0.8, 0.2, 0.6, 0.9]    # 权重高的其对应的下表抽取的机会高。 注意：抽的下标，权重知识表示抽到的概率\n",
    "wr_sampler = WeightedRandomSampler(w, 4, False)    # 抽取10个\n",
    "for x in wr_sampler:\n",
    "    print(x)\n",
    "print(\"------------------------\")    \n",
    "loader = DataLoader(train_mnist, sampler=wr_sampler, batch_size=100, drop_last=False)  \n",
    "for d, t in loader:\n",
    "    print(d.shape, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. BatchSampler\n",
    "    - 这个用于批次采样器，不用这样本采样器。参数包含：\n",
    "        - sampler (Sampler) – 产生采样下标集合.\n",
    "        - batch_size (python:int) – 批次大小.\n",
    "        - drop_last (bool) - 剩余的不足的是否保留。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 3]\n",
      "[0]\n",
      "------------------------\n",
      "torch.Size([3, 1, 28, 28]) torch.Size([3])\n",
      "torch.Size([1, 1, 28, 28]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "w = [0.4, 0.8, 0.2, 0.6, 0.9]   \n",
    "wr_sampler = WeightedRandomSampler(w, 4, False) \n",
    "\n",
    "b_sampler = BatchSampler(wr_sampler, batch_size=3, drop_last=False)\n",
    "\n",
    "for x in b_sampler:\n",
    "    print(x)\n",
    "print(\"------------------------\")    \n",
    "loader = DataLoader(train_mnist, batch_sampler=b_sampler)  \n",
    "for d, t in loader:\n",
    "    print(d.shape, t.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. DistributedSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用于分布式训练\n",
    "    dataset, \n",
    "    num_replicas=None,    # 进程数\n",
    "    rank=None,            # 进程排名 \n",
    "    shuffle=True\n",
    "- 因为需要分布式的package，所以这里暂时不演示代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## worker的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 指定num_workers参数，可以启动多进程加载数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n",
      "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n",
      "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n",
      "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n",
      "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n",
      "torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "train_mnist = MNIST(root=\"./datasets\", train=True, download=False,transform=ToTensor())\n",
    "\n",
    "loader = DataLoader(train_mnist, batch_size=10000, shuffle=True, drop_last=False, num_workers=2)\n",
    "for d, t in loader:    # 数据与标签\n",
    "    print(d.shape, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. worker_init_fn 指定每个进程的处理与初始化\n",
    "    - 用来设置每个进程的状态；\n",
    "    - 这个函数会通过参数传递一个pid（进程id）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 13772, 14988, 21548) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    104\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-27db770b1b36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mnist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworker_init_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworker_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# 数据与标签\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    806\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 808\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    809\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 13772, 14988, 21548) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "train_mnist = MNIST(root=\"./datasets\", train=True, download=False,transform=ToTensor())\n",
    "\n",
    "def worker_fn(w_id):\n",
    "    print(F\"进程：{w_id}\")\n",
    "\n",
    "loader = DataLoader(train_mnist, batch_size=10000, shuffle=True, drop_last=False, num_workers=3, worker_init_fn=worker_fn)\n",
    "for d, t in loader:    # 数据与标签\n",
    "    print(d.shape, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 说明：\n",
    "    - 上面的错误来自平台的缘故：非Window平台才能执行；\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 源代码注释中的说明：\n",
    "    - Window中不支持SIGCHLD信号，这个是典型的僵尸进程的处理方式。\n",
    "```python\n",
    "        # This raises a `RuntimeError` if any worker died expectedly. This error\n",
    "        # can come from either the SIGCHLD handler in `_utils/signal_handling.py`\n",
    "        # (only for non-Windows platforms), or the manual check below on errors\n",
    "        # and timeouts.\n",
    "        #\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. collate_fn协作函数的处理：\n",
    "    - 这个函数会传递一个batch参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 9988, 13420, 8472) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 761\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    762\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    104\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e9e626ed1fcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mnist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# 数据与标签\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    806\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 808\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    809\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 9988, 13420, 8472) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "train_mnist = MNIST(root=\"./datasets\", train=True, download=False,transform=ToTensor())\n",
    "\n",
    "def collate_fn(batch):\n",
    "    print(\"协作:\", type(batch))\n",
    "    return batch\n",
    "\n",
    "loader = DataLoader(train_mnist, batch_size=10000, shuffle=True, drop_last=False, num_workers=3, collate_fn=collate_fn)\n",
    "for d, t in loader:    # 数据与标签\n",
    "    print(d.shape, t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这个错误也是因为Window平台的缘故。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
