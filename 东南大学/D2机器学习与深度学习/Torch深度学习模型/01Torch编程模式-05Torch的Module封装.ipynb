{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 下面是一个利用梯度直接实现的LeNet-5的手写数字识别程序，其中的核心部分，在本文采用封装的方式实现。实际上Torch的不仅仅封装，还做了很多优化的操作，这里列出代码，仅仅是理解与说明Module与Layer的封装原理；以及每个封装的模块封装的主要功能与操作。\n",
    "- 为了计算时间，只训练了100轮，每轮的训练样本分成6个批次，每个批次10000个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第000轮\n",
      "\t损失值：2.358580\t测试集准确率： 15.48%\n",
      "第010轮\n",
      "\t损失值：0.888810\t测试集准确率： 73.71%\n",
      "第020轮\n",
      "\t损失值：0.526765\t测试集准确率： 83.67%\n",
      "第030轮\n",
      "\t损失值：0.414707\t测试集准确率： 87.15%\n",
      "第040轮\n",
      "\t损失值：0.345243\t测试集准确率： 89.41%\n",
      "第050轮\n",
      "\t损失值：0.307811\t测试集准确率： 90.55%\n",
      "第060轮\n",
      "\t损失值：0.277965\t测试集准确率： 91.49%\n",
      "第070轮\n",
      "\t损失值：0.254911\t测试集准确率： 92.05%\n",
      "第080轮\n",
      "\t损失值：0.236175\t测试集准确率： 92.63%\n",
      "第090轮\n",
      "\t损失值：0.220653\t测试集准确率： 93.08%\n",
      "------训练完毕------\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# 读取图片\n",
    "def load_image_fromfile(filename):\n",
    "    with open(filename, 'br') as fd:\n",
    "        # 读取图像的信息\n",
    "        header_buf = fd.read(16)   # 16字节，4个int整数\n",
    "        # 按照字节解析头信息（具体参考python SL的struct帮助）\n",
    "        magic_, nums_, width_, height_ = struct.unpack('>iiii', header_buf)  # 解析成四个整数：>表示大端字节序，i表示4字节整数\n",
    "        # 保存成ndarray对象\n",
    "        imgs_ = np.fromfile(fd, dtype=np.uint8)\n",
    "        imgs_ = imgs_.reshape(nums_, height_, width_)\n",
    "    return imgs_\n",
    "\n",
    "# 读取标签\n",
    "def load_label_fromfile(filename):\n",
    "    with open(filename, 'br') as fd:\n",
    "        header_buf = fd.read(8) \n",
    "        magic, nums = struct.unpack('>ii' ,header_buf) \n",
    "        labels_ = np.fromfile(fd, np.uint8) \n",
    "    return labels_\n",
    "\n",
    "# 读取训练集\n",
    "train_x = load_image_fromfile(\"datasets/train-images.idx3-ubyte\")\n",
    "train_y = load_label_fromfile(\"datasets/train-labels.idx1-ubyte\")\n",
    "train_x = train_x.astype(np.float64)\n",
    "train_y = train_y.astype(np.int64)\n",
    "# 读取测试集\n",
    "test_x = load_image_fromfile(\"datasets/t10k-images.idx3-ubyte\")\n",
    "test_y = load_label_fromfile(\"datasets/t10k-labels.idx1-ubyte\")\n",
    "\n",
    "# 1. 定义训练参数（3层卷积，2层全链接）\n",
    "#   1.1. 1 @28 * 28 ->   6@28 * 28:  6@5*5 的卷积核  -> 6 @ 14 * 14(池化)\n",
    "#   1.2. 6@14 * 14 ->   16@10 * 10:  16@5*5 的卷积核  -> 10 @ 5 * 5(池化)\n",
    "#   1.3. 16@5 * 5 ->   120@1 * 1:  120@5*5 的卷积核  （没有池化）\n",
    "#   1.4. 120 * 84  \n",
    "#   1.5. 84 * 10   (输出10个特征，分类0-9十个数字)\n",
    "# 1.1\n",
    "w_6_5_5 = torch.Tensor(6, 1, 5, 5)   #（C_out, C_in, H_k, W_k）\n",
    "b_6_5_5 = torch.Tensor(6)               # (C_out)   # 卷积核也可以不使用偏置项的\n",
    "# 初始化（模仿Torch的源代码，自己采用正态分布，每次计算都是无穷大）\n",
    "stdv = 1.0 / math.sqrt(1 * 5 * 5)\n",
    "w_6_5_5.data.uniform_(-stdv, stdv)\n",
    "b_6_5_5.data.uniform_(-stdv, stdv)\n",
    "# print(w_6_5_5)\n",
    "\n",
    "# 1.2 \n",
    "w_16_5_5 = torch.Tensor(16, 6, 5, 5)\n",
    "b_16_5_5 = torch.Tensor(16)\n",
    "# 初始化\n",
    "stdv = 1.0 / math.sqrt(6 * 5 * 5)\n",
    "w_16_5_5.data.uniform_(-stdv, stdv)\n",
    "b_16_5_5.data.uniform_(-stdv, stdv)\n",
    "\n",
    "# 1.3 \n",
    "w_120_5_5 = torch.Tensor(120, 16, 5, 5)\n",
    "b_120_5_5 = torch.Tensor(120) \n",
    "# 初始化\n",
    "stdv = 1.0 / math.sqrt(16 * 5 * 5)\n",
    "w_120_5_5.data.uniform_(-stdv, stdv)\n",
    "b_120_5_5.data.uniform_(-stdv, stdv)\n",
    "\n",
    "# 1.4\n",
    "w_120_84 = torch.Tensor(84, 120) \n",
    "b_120_84 = torch.Tensor(84) \n",
    "# 初始化\n",
    "stdv = 1.0 / math.sqrt(120)   # 使用输入的特征数作为均匀分布的计算基数\n",
    "w_120_84.data.uniform_(-stdv, stdv)\n",
    "b_120_84.data.uniform_(-stdv, stdv)\n",
    "\n",
    "# 1.5\n",
    "w_84_10 =torch.Tensor(10, 84) \n",
    "b_84_10 = torch.Tensor(10)\n",
    "# 初始化\n",
    "stdv = 1.0 / math.sqrt(84)\n",
    "w_84_10.data.uniform_(-stdv, stdv)\n",
    "b_84_10.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "# print(w_16_5_5)\n",
    "w_6_5_5.requires_grad = True\n",
    "b_6_5_5.requires_grad = True\n",
    "# 1.2 \n",
    "w_16_5_5.requires_grad = True\n",
    "b_16_5_5.requires_grad = True\n",
    "# 1.3 \n",
    "w_120_5_5.requires_grad = True\n",
    "b_120_5_5.requires_grad = True\n",
    "# 1.4\n",
    "w_120_84.requires_grad = True\n",
    "b_120_84.requires_grad = True\n",
    "# 1.5\n",
    "w_84_10.requires_grad = True\n",
    "b_84_10.requires_grad = True\n",
    "\n",
    "# 2. 定义forward模型(为了反复调用，封装成函数)\n",
    "@torch.enable_grad()\n",
    "def lenet5_forward(input):\n",
    "    \"\"\"\n",
    "    input的格式：4-D（N, 1, 28, 28）：N表示每批次的样本数量\n",
    "    out的格式：与input相同4-D（N, 10）：N表示每批次的样本数量\n",
    "    \"\"\"\n",
    "    # 1.1 \n",
    "    o_c1 = torch.nn.functional.conv2d(input=input, weight=w_6_5_5, bias=b_6_5_5, padding = 2)  # 原始图像28*28\n",
    "    o_a1 = torch.nn.functional.relu(o_c1)\n",
    "    o_p1 = torch.nn.functional.max_pool2d(input= o_a1, kernel_size=(2,2))\n",
    "    o1 = o_p1\n",
    "    # 1.2\n",
    "    o_c2 = torch.nn.functional.conv2d(input=o1, weight=w_16_5_5, bias=b_16_5_5)\n",
    "    o_a2 = torch.nn.functional.relu(o_c2)\n",
    "    o_p2 = torch.nn.functional.max_pool2d(input= o_a2, kernel_size=(2,2))\n",
    "    o2 = o_p2\n",
    "    # 1.3\n",
    "    o_c3 = torch.nn.functional.conv2d(input=o2, weight=w_120_5_5, bias=b_120_5_5)\n",
    "    o_a3 = torch.nn.functional.relu(o_c3)\n",
    "    # 无池化\n",
    "    # o3 = o_a3.squeeze()    # 格式转换(把最后的1*1直接降维掉），转换为60000 * 120\n",
    "    o3 = o_a3.view(o_a3.shape[0], o_a3.shape[1])\n",
    "    # 1.4\n",
    "    o_c4 = torch.nn.functional.linear(o3, w_120_84, b_120_84)\n",
    "    o_a4 = torch.nn.functional.relu(o_c4)\n",
    "    o4 = o_a4\n",
    "    # 1.5\n",
    "    o_c5 = torch.nn.functional.linear(o4, w_84_10, b_84_10)\n",
    "    o_a5 = torch.log_softmax(o_c5, dim=1)\n",
    "    o5 = o_a5\n",
    "    return  o5 \n",
    "\n",
    "# 3. 定义损失模型（封装成函数）\n",
    "@torch.enable_grad()\n",
    "def loss_model(out, target):\n",
    "    loss_ = torch.nn.functional.cross_entropy(out, target)\n",
    "    return loss_\n",
    "\n",
    "\n",
    "# 为了速度取1000个样本训练\n",
    "# train_x = train_x[0:10]\n",
    "# train_y = train_y[0:10]\n",
    "# 训练集\n",
    "x = torch.Tensor(train_x).view(train_x.shape[0], 1, train_x.shape[1], train_x.shape[2])   # N,C,W,H\n",
    "y = torch.LongTensor(train_y)\n",
    "# # 测试集\n",
    "t_x =  torch.Tensor(test_x).view(test_x.shape[0], 1, test_x.shape[1], test_x.shape[2])   # N,C,W,H\n",
    "t_y =  torch.LongTensor(test_y)\n",
    "\n",
    "# 训练超参数\n",
    "# 学习率\n",
    "learn_rate = 0.001\n",
    "# 训练轮数\n",
    "epoch = 100\n",
    "# 没批样本数\n",
    "batch_size = 10000\n",
    "# 批次计算\n",
    "batch_num = len(train_y) // batch_size\n",
    "\n",
    "# 轮次循环\n",
    "for e in range(epoch):\n",
    "    # 批次循环\n",
    "    for idx in range(batch_num):\n",
    "        # 批次样本\n",
    "        start = idx *batch_size\n",
    "        end = (idx + 1) * batch_size\n",
    "        b_x = x[start: end]\n",
    "        b_y = y[start: end]\n",
    "        # 计算输出\n",
    "        b_y_ = lenet5_forward(b_x)\n",
    "#         break\n",
    "        # 计算损失\n",
    "        l_ = loss_model(b_y_, b_y)\n",
    "        # 计算梯度\n",
    "        l_.backward(retain_graph=True)\n",
    "#         print(w_6_5_5.grad)\n",
    "        # 梯度更新(使用上下文管理器，进制对运算实现图跟踪)\n",
    "        with torch.autograd.no_grad():\n",
    "            w_6_5_5 -= learn_rate * w_6_5_5.grad\n",
    "            b_6_5_5 -= learn_rate * b_6_5_5.grad\n",
    "\n",
    "            w_16_5_5 -= learn_rate * w_16_5_5.grad\n",
    "            b_16_5_5 -= learn_rate * b_16_5_5.grad\n",
    "\n",
    "            w_120_5_5 -= learn_rate * w_120_5_5.grad\n",
    "            b_120_5_5 -= learn_rate * b_120_5_5.grad\n",
    "\n",
    "            w_120_84 -= learn_rate * w_120_84.grad\n",
    "            b_120_84 -= learn_rate * b_120_84.grad\n",
    "            \n",
    "            w_84_10 -= learn_rate * w_84_10.grad\n",
    "            b_84_10 -= learn_rate * b_84_10.grad\n",
    "            \n",
    "            # 复原梯度\n",
    "            w_6_5_5.grad.zero_()\n",
    "            b_6_5_5.grad.zero_()\n",
    "            \n",
    "            w_16_5_5.grad.zero_()\n",
    "            b_16_5_5.grad.zero_()\n",
    "            \n",
    "            w_120_5_5.grad.zero_()\n",
    "            b_120_5_5.grad.zero_()\n",
    "            \n",
    "            w_120_84.grad.zero_()\n",
    "            b_120_84.grad.zero_()\n",
    "            \n",
    "            w_84_10.grad.zero_()\n",
    "            b_84_10.grad.zero_()\n",
    "            \n",
    "    # 每一轮次完毕，输出损失度与测试集准确率\n",
    "    if e % 10 ==0:\n",
    "        print(F\"第{e:03d}轮\")\n",
    "        print(F\"\\t损失值：{l_:8.6f}\",end=\"\")  \n",
    "        # 测试集测试\n",
    "        with torch.autograd.no_grad():   \n",
    "            predict = lenet5_forward(t_x)\n",
    "            # 计算准确率\n",
    "            y_ = predict.argmax(dim=1)\n",
    "            correct_rate = (y_ == t_y).float().mean()\n",
    "            print(F\"\\t测试集准确率：{correct_rate*100: 6.2f}%\")\n",
    "print(\"------训练完毕------\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 上面训练过程的核心是：\n",
    "    1. 决策模型输出：\n",
    "        - `b_y_ = lenet5_forward(b_x)`\n",
    "    2. 损失计算：\n",
    "        - `l_ = loss_model(b_y_, b_y)`\n",
    "    3. 梯度计算：\n",
    "        - `l_.backward(retain_graph=True)`\n",
    "    4. 梯度更新：\n",
    "        - `w_6_5_5 -= learn_rate * w_6_5_5.grad`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 容器与Module封装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对其中决策运算的封装就是Module类，这个类主要提供的是一种封装框架，具体的决策过程实现需要根据需求个性化来实现，这个封装的好处就是利用Python的可调用对象实现决策计算输出，并在调用的时候隐藏繁琐的实现细节，实现简洁的训练过程调用（与我们封装成函数的目的一样，只是封装成函数比较low一点）。\n",
    "    - m = torch.nn.Module()\n",
    "    - result = m(input)     # 预测结果，或者输入样本的计算结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module类的基本使用模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Module的使用模式由其设计决定，一般使用模式两个要素：\n",
    "    - 通过继承Module，重载forward函数，实现决策输出运算；\n",
    "        - 在构造器`__init__`函数中提供运算需要的初始化对象；\n",
    "        - forward的参数根据使用自己确定；\n",
    "    - 调用Module完成决策模型运算。\n",
    "\n",
    "- 下面是Module的使用例子代码   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策输出计算结果\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class  PerceptronModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptronModule, self).__init__() \n",
    "        # 初始化\n",
    "        \n",
    "    def forward(self):\n",
    "        return \"决策输出计算结果\"\n",
    "\n",
    "\n",
    "m = PerceptronModule()\n",
    "\n",
    "result = m()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运算过程及其管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Module对运算过程的管理包含：\n",
    "    1. 训练参数的定义\n",
    "    2. 训练参数的属性设置\n",
    "    3. 训练参数的初始化\n",
    "    4. 决策计算过程\n",
    "    5. 决策过程求导的处理（中间变量的导数）\n",
    "    \n",
    "- 下面先看看运算过程，例子代码是感知器的实现。\n",
    "    - 其中所有的管理采用过程式管理，所有管理都是单独管理（需要直接访问变量来管理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x139692eb8>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class  PerceptronModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptronModule, self).__init__() \n",
    "        # 1. 需要训练的参数定义\n",
    "        self.w = torch.Tensor(1, 4) \n",
    "        self.b = torch.Tensor(1) \n",
    "        \n",
    "        # 2. 需要训练的参数设置\n",
    "        self.w.requires_grad = True\n",
    "        self.b.requires_grad = True\n",
    "        \n",
    "        # 3. 需要训练的参数初始化\n",
    "        stdv = 1.0 / math.sqrt(4)   \n",
    "        self.w.data.uniform_(-stdv, stdv)\n",
    "        self.b.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 4. 计算过程的实现\n",
    "        out = torch.nn.functional.linear(x, self.w, self.b)\n",
    "        y_ = torch.sigmoid(out)\n",
    "        return y_\n",
    "\n",
    "\n",
    "import sklearn.datasets\n",
    "# 加载鸢尾花数据集\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "x = torch.Tensor(data[0:100])\n",
    "y = torch.Tensor(target[0:100]).view(100, 1)\n",
    "\n",
    "# Module对象\n",
    "m = PerceptronModule()\n",
    "# Module对象调用\n",
    "y_ = m(x)\n",
    "\n",
    "# 使用Module计算损失函数\n",
    "loss = torch.nn.functional.mse_loss(y_, y)\n",
    "\n",
    "# 优化损失-求导\n",
    "loss.backward()\n",
    "\n",
    "print(m.parameters())\n",
    "print(list(m.parameters()))   # 采用非模块方式，对参数，求导等管理比较麻烦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 子模块管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为了便于管理每个运算过程，采用子模块管理每个运算过程。而不是像上面采用的非模块化封装。\n",
    "    - 把一个运算过程设计成一个独立的对象\n",
    "        - Function：无训练参数\n",
    "        - Layer：有训练参数（通过访问Layer对象的属性管理）\n",
    "- 下面是采用子模块管理的例子代码\n",
    "    - 采用子模块管理的好处就是通过模块名，可以管理相关的所有参数、数据与数据计算过程；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x135637990>\n",
      "[Parameter containing:\n",
      "tensor([[-0.4140, -0.1299, -0.0851, -0.1636]], requires_grad=True), Parameter containing:\n",
      "tensor([0.1850], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class  PerceptronModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptronModule, self).__init__() \n",
    "        # 1. 子模块对象\n",
    "        # （简单的定义隐藏了参数定义，参数属性，参数初始化，运算过程）\n",
    "        self.layer = torch.nn.Linear(4, 1)    # 4 输入特征，3 输出特征\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 2. 子模块计算过程的实现\n",
    "        out = self.layer(x)   # 可调用对象\n",
    "        y_ = torch.sigmoid(out)\n",
    "        return y_\n",
    "\n",
    "\n",
    "import sklearn.datasets\n",
    "# 加载鸢尾花数据集\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "x = torch.Tensor(data[0:100])\n",
    "y = torch.Tensor(target[0:100]).view(100, 1)\n",
    "\n",
    "# Module对象\n",
    "m = PerceptronModule()\n",
    "# Module对象调用\n",
    "y_ = m(x)\n",
    "\n",
    "# 使用Module计算损失函数\n",
    "loss = torch.nn.functional.mse_loss(y_, y)\n",
    "\n",
    "# 优化损失-求导\n",
    "loss.backward()\n",
    "\n",
    "print(m.parameters())\n",
    "print(list(m.parameters()))   # 子模块化方式：参数定义，属性设置，初始化都已经没子模块封装实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 函数子模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 其中没有参数的函数采用不同的封装方式Function：\n",
    "    - 主要管理求导\n",
    "    \n",
    "- 下面是例子代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4384, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class  PerceptronModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptronModule, self).__init__() \n",
    "        # 1. 子模块对象\n",
    "        # （简单的定义隐藏了参数定义，参数属性，参数初始化，运算过程）\n",
    "        self.layer = torch.nn.Linear(4, 1)    # 4 输入特征，3 输出特征\n",
    "        # ------------------\n",
    "        self.sigmoid = torch.nn.Sigmoid()   # 函数对象\n",
    "        # ------------------\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 2. 子模块计算过程的实现\n",
    "        out = self.layer(x)   # 可调用对象\n",
    "        # ---------------------\n",
    "        y_ = self.sigmoid(out)\n",
    "        # ---------------------\n",
    "        return y_\n",
    "\n",
    "\n",
    "import sklearn.datasets\n",
    "# 加载鸢尾花数据集\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "x = torch.Tensor(data[0:100])\n",
    "y = torch.Tensor(target[0:100]).view(100, 1)\n",
    "\n",
    "# Module对象\n",
    "m = PerceptronModule()\n",
    "# Module对象调用\n",
    "y_ = m(x)\n",
    "\n",
    "# 使用Module计算损失函数 ------------------------\n",
    "l = torch.nn.MSELoss()     # 函数对象\n",
    "loss = l(y_, y) \n",
    "# --------------------------------------------\n",
    "\n",
    "# 优化损失-求导\n",
    "loss.backward()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练参数管理与优化器\n",
    "\n",
    "- 训练参数管理主要包含两个：\n",
    "    1. 梯度更新；\n",
    "    2. 上次梯度清零；\n",
    "    \n",
    "- 这两个管理被封装到优化器中\n",
    "    - 当然优化器还有其他的优化处理：比如学习率的动态变化等（可以加速提升训练过程）\n",
    "    \n",
    "- 下面是优化器的核心功能使用例子代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失值： 0.4592949\t测试准确度： 0.0\n",
      "损失值： 0.38074505\t测试准确度： 0.02\n",
      "损失值： 0.31565797\t测试准确度： 0.42\n",
      "损失值： 0.257681\t测试准确度： 0.44\n",
      "损失值： 0.20784816\t测试准确度： 0.88\n",
      "损失值： 0.16737387\t测试准确度： 0.99\n",
      "损失值： 0.13562103\t测试准确度： 0.99\n",
      "损失值： 0.11111626\t测试准确度： 1.0\n",
      "损失值： 0.092255495\t测试准确度： 1.0\n",
      "损失值： 0.07765004\t测试准确度： 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class  PerceptronModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptronModule, self).__init__() \n",
    "        # 1. 子模块对象\n",
    "        # （简单的定义隐藏了参数定义，参数属性，参数初始化，运算过程）\n",
    "        self.layer = torch.nn.Linear(4, 1)    # 4 输入特征，3 输出特征\n",
    "        self.sigmoid = torch.nn.Sigmoid()   # 函数对象\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 2. 子模块计算过程的实现\n",
    "        out = self.layer(x)   # 可调用对象\n",
    "        y_ = self.sigmoid(out)\n",
    "        return y_\n",
    "\n",
    "\n",
    "import sklearn.datasets\n",
    "# 加载鸢尾花数据集\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "x = torch.Tensor(data[0:100])\n",
    "y = torch.Tensor(target[0:100]).view(100, 1)\n",
    "\n",
    "epoch = 200\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Module对象\n",
    "m = PerceptronModule()\n",
    "l = torch.nn.MSELoss()     # 函数对象\n",
    "# 优化器对象\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)    # 构造器中需要模块的参数\n",
    "# Module对象调用\n",
    "for e in range(epoch):\n",
    "    # 1. 梯度清零 --------------------------\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    y_ = m(x)\n",
    "    loss = l(y_, y) \n",
    "    # 优化损失-求导\n",
    "    loss.backward()\n",
    "    \n",
    "    # 2. 梯度更新--------------------------\n",
    "    optimizer.step()\n",
    "    with torch.autograd.no_grad():\n",
    "        if e % 20 ==0:\n",
    "            print(\"损失值：\", loss.detach().numpy(), end=\"\")\n",
    "            predict = m(x)\n",
    "            predict[predict>0.5] =1\n",
    "            predict[predict<=0.5] =0\n",
    "            print(\"\\t测试准确度：\", (predict == y).float().mean().detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer，函数与Module模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 函数模块的继承结构\n",
    "\n",
    "- 使用MSELoss函数模块作为例子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `MSELoss`\n",
    "    - `_Loss`\n",
    "        - `torch.nn.modules.module.Module`\n",
    "            - `builtins.object`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer模块的继承结构\n",
    "\n",
    "- 使用Liner Layer模块作为例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Linear`\n",
    "    - `torch.nn.modules.module.Module`\n",
    "    - `builtins.object`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module模块\n",
    "\n",
    "- 从上面的继承结构看出，函数模块与Layer模块都是Module，采用树状结构管理。\n",
    "    - 函数模块比较简单：不带训练参数\n",
    "    - Layer模块带训练参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Module类提供了数据结构管理\n",
    "    1. 子模块的管理；\n",
    "    2. 子模块的运算与运算过程管理；\n",
    "    3. 子模块的求导逆向管理；\n",
    "    4. 子模块训练参数管理；\n",
    "    5. 子模块非训练参数管理；\n",
    "    6. GPU/CPU计算方式切换管理；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 子模块管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 子模块管理管理主要是数据结构的基本操作，子模块不负责形成决策运算，这个是forward函数负责。\n",
    "    1. 添加子模块：\n",
    "        - `add_module(self, name, module)`\n",
    "    2. 返回子模块：\n",
    "        - `children(self)`\n",
    "        - `modules(self)`\n",
    "        - `named_children(self)`\n",
    "        - `named_modules(self, memo=None, prefix='')`\n",
    "    3. 对某些模块的模式设置(主要是某些子模块的参数)\n",
    "        - `eval(self)`\n",
    "        - `train(self, mode=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 下面是子模块的管理的例子\n",
    "    - 使用self定义成员属性，创建的都会纳入子模块管理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'> [Linear(in_features=4, out_features=1, bias=True), Sigmoid()]\n",
      "------------------\n",
      "<class 'generator'> [PerceptronModule(\n",
      "  (layer): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "), Linear(in_features=4, out_features=1, bias=True), Sigmoid()]\n",
      "------------------\n",
      "[('layer', Linear(in_features=4, out_features=1, bias=True)), ('sigmoid', Sigmoid())]\n",
      "------------------\n",
      "[('', PerceptronModule(\n",
      "  (layer): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")), ('layer', Linear(in_features=4, out_features=1, bias=True)), ('sigmoid', Sigmoid())]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class  PerceptronModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptronModule, self).__init__() \n",
    "        # 1. 子模块对象\n",
    "        # （简单的定义隐藏了参数定义，参数属性，参数初始化，运算过程）\n",
    "        self.layer = torch.nn.Linear(4, 1)    # 4 输入特征，3 输出特征\n",
    "        self.sigmoid = torch.nn.Sigmoid()   # 函数对象\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 2. 子模块计算过程的实现\n",
    "        out = self.layer(x)   # 可调用对象\n",
    "        y_ = self.sigmoid(out)\n",
    "        return y_\n",
    "\n",
    "\n",
    "# Module对象\n",
    "m = PerceptronModule()\n",
    "\n",
    "# children成员函数\n",
    "print(type(m.children()) ,list(m.children()))   # 两个子模块：返回的格式是生成器类型Generator\n",
    "# modules成员函数：返回迭代器\n",
    "print(\"------------------\")\n",
    "print(type(m.modules()), list(m.modules()))   # 返回所有的模块，包括PerceptronModule本身。\n",
    "# named_children成员函数\n",
    "print(\"------------------\")\n",
    "print(list(m.named_children()))\n",
    "# named_modules成员函数\n",
    "print(\"------------------\")\n",
    "print(list(m.named_modules()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 子模块的添加与应用\n",
    "    - 可以通过`add_module`添加，在forward中调用；（尽管不推荐，但在某些时候还是可以解决某些问题）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9001],\n",
      "        [0.8923]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class  PerceptronModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptronModule, self).__init__() \n",
    "\n",
    "    def forward(self, x):   # 1. 先试用\n",
    "        # 2. 子模块计算过程的实现\n",
    "        out = self.layer(x)   # 可调用对象\n",
    "        y_ = self.sigmoid(out)\n",
    "        return y_\n",
    "\n",
    "\n",
    "# Module对象\n",
    "m = PerceptronModule()\n",
    "layer1 = torch.nn.Linear(4, 1)    # 4 输入特征，3 输出特征\n",
    "layer2 = torch.nn.Sigmoid()   # 函数对象\n",
    "m.add_module(\"layer\", layer1)     # 后添加\n",
    "m.add_module(\"sigmoid\", layer2)\n",
    "\n",
    "import sklearn.datasets\n",
    "# 加载鸢尾花数据集\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "x = torch.Tensor(data[0:100])\n",
    "y = torch.Tensor(target[0:100]).view(100, 1)\n",
    "\n",
    "# Module对象调用\n",
    "y_ = m(x[0:2])\n",
    "print(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 子模块的运算与运算过程管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 子模块的运算过程逻辑通过重载forward函数实现。\n",
    "- 调用通过模块的可调用运算符实现()\n",
    "    - `__call__`\n",
    "- Module提供了回调的钩子机制：\n",
    "    - `forward`调用后被回调：\n",
    "        - `register_forward_hook(self, hook)`\n",
    "            - 钩子函数原型：`hook(module, input, output) -> None or modified output`\n",
    "    - `forward`被调用前回调：\n",
    "        - `register_forward_pre_hook(self, hook)`\n",
    "            - `hook(module, input) -> None or modified input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before============\n",
      "PerceptronModule(\n",
      "  (layer): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "(tensor([[5.1000, 3.5000, 1.4000, 0.2000],\n",
      "        [4.9000, 3.0000, 1.4000, 0.2000]]),)\n",
      "after============\n",
      "PerceptronModule(\n",
      "  (layer): Linear(in_features=4, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "(tensor([[5.1000, 3.5000, 1.4000, 0.2000],\n",
      "        [4.9000, 3.0000, 1.4000, 0.2000]]),)\n",
      "tensor([[0.3483],\n",
      "        [0.3723]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.3483],\n",
      "        [0.3723]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class  PerceptronModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptronModule, self).__init__() \n",
    "\n",
    "    def forward(self, x):   # 1. 先试用\n",
    "        # 2. 子模块计算过程的实现\n",
    "        out = self.layer(x)   # 可调用对象\n",
    "        y_ = self.sigmoid(out)\n",
    "        return y_\n",
    "\n",
    "\n",
    "# Module对象\n",
    "m = PerceptronModule()\n",
    "layer1 = torch.nn.Linear(4, 1)    # 4 输入特征，3 输出特征\n",
    "layer2 = torch.nn.Sigmoid()   # 函数对象\n",
    "m.add_module(\"layer\", layer1)     # 后添加\n",
    "m.add_module(\"sigmoid\", layer2)\n",
    "\n",
    "# ===========注册钩子函数\n",
    "def after_hook(module, input, output):\n",
    "    print(\"after============\")\n",
    "    print(module)\n",
    "    print(input)\n",
    "    print(output)\n",
    "    # 如果想对输出做进一步处理，可以在这儿处理（比如其中view转换）\n",
    "    return None   #-> None or modified output\n",
    "def before_hook(module, input):\n",
    "    print(\"before============\")\n",
    "    print(module)\n",
    "    print(input)\n",
    "    # 如果想对输出做进一步处理，可以在这儿处理（比如其中view转换）\n",
    "    return None   #-> None or modified output\n",
    "\n",
    "m.register_forward_hook(after_hook)\n",
    "m.register_forward_pre_hook(before_hook)\n",
    "# ==========================\n",
    "\n",
    "import sklearn.datasets\n",
    "# 加载鸢尾花数据集\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "x = torch.Tensor(data[0:100])\n",
    "y = torch.Tensor(target[0:100]).view(100, 1)\n",
    "\n",
    "# Module对象调用\n",
    "y_ = m(x[0:2])\n",
    "print(y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 子模块的求导逆向管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 提供了一个钩子函数，用来处理求导过程中的导数传递：\n",
    "    - `register_backward_hook(self, hook)`\n",
    "         - 钩子函数原型：`hook(module, grad_input, grad_output) -> Tensor or None`\n",
    "             - 不需要直接修改这两个参数，而是返回新的梯度，返回的新的梯度会用作下一次的梯度输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `register_backward_hook`函数是对整个Module的求导回调\n",
    "    - 文档说：如果需要中间的梯度，则还是使用Tensor的钩子函数来获取争对某个变量的导数（可以参考我们前面的文档）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对非子模块的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入梯度： (tensor([4., 5., 6.]), tensor([ 3.,  8., 15.]))\n",
      "输出梯度： (tensor(1.),)\n",
      "tensor([12., 20., 30.])\n",
      "tensor([ 4., 10., 18.])\n",
      "tensor([ 3.,  8., 15.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class  PerceptronModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptronModule, self).__init__() \n",
    "\n",
    "    def forward(self, x, y, z):   # 1. 先试用\n",
    "        xy = x * y\n",
    "        xyz = torch.dot(xy, z)\n",
    "        return xyz\n",
    "\n",
    "\n",
    "# Module对象\n",
    "m = PerceptronModule()\n",
    "# ==========注册钩子函数\n",
    "def back_hook(module, grad_input, grad_output):  \n",
    "    print(\"输入梯度：\", grad_input)\n",
    "    print(\"输出梯度：\", grad_output)\n",
    "    return None\n",
    "m.register_backward_hook(back_hook)\n",
    "# ====================\n",
    "\n",
    "x = torch.Tensor([1, 2, 3]) \n",
    "y = torch.Tensor([3, 4, 5]) \n",
    "z = torch.Tensor([4, 5, 6])\n",
    "x.requires_grad=True\n",
    "y.requires_grad=True\n",
    "z.requires_grad=True\n",
    "\n",
    "\n",
    "y_ = m(x, y, z)\n",
    "y_.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)\n",
    "\n",
    "# 输出的是最后点积的导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 子模块的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入梯度： <class 'tuple'> 1\n",
      "输出梯度： <class 'tuple'> 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class  PerceptronModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptronModule, self).__init__() \n",
    "        # 1. 子模块对象\n",
    "        # （简单的定义隐藏了参数定义，参数属性，参数初始化，运算过程）\n",
    "        self.layer = torch.nn.Linear(4, 1)    # 4 输入特征，3 输出特征\n",
    "        self.sigmoid = torch.nn.Sigmoid()   # 函数对象\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 2. 子模块计算过程的实现\n",
    "        out = self.layer(x)   # 可调用对象\n",
    "        y_ = self.sigmoid(out)\n",
    "        return y_\n",
    "\n",
    "\n",
    "import sklearn.datasets\n",
    "\n",
    "# 加载鸢尾花数据集\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "x = torch.Tensor(data[0:100])\n",
    "y = torch.Tensor(target[0:100]).view(100, 1)\n",
    "\n",
    "epoch = 200\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Module对象\n",
    "m = PerceptronModule()\n",
    "# ==========注册钩子函数\n",
    "def back_hook(module, grad_input, grad_output):  \n",
    "    print(\"输入梯度：\", type(grad_input), len(grad_input))\n",
    "    print(\"输出梯度：\", type(grad_output), len(grad_output))\n",
    "    return None\n",
    "m.register_backward_hook(back_hook)\n",
    "# ====================\n",
    "l = torch.nn.MSELoss()     # 函数对象\n",
    "y_ = m(x)\n",
    "loss = l(y_, y) \n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 子模块训练参数管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练参数的获取\n",
    "\n",
    "- 训练参数在使用子模块是自动管理的，封装的优化器需要使用的参数，就通过下面的函数获取，可以使用如下函数获取：\n",
    "\n",
    "```python\n",
    "\n",
    "    parameters(self, recurse=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数的初始化\n",
    "\n",
    "- 默认情况下训练参数的初始化都是模型的，但是用户需要初始化，可以指定函数来完成，函数的指定使用如下函数：\n",
    "\n",
    "```python\n",
    "\n",
    "    apply(self, fn)  \n",
    "```\n",
    "\n",
    "- 参数说明：\n",
    "    - fn的函数原型是：`fn (Module) -> None`\n",
    "        - 参数是Module，不需要返回值；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv2D模块中参数的默认初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
    "                 padding, dilation, transposed, output_padding,\n",
    "                 groups, bias, padding_mode):\n",
    "        super(_ConvNd, self).__init__()\n",
    "        if in_channels % groups != 0:\n",
    "            raise ValueError('in_channels must be divisible by groups')\n",
    "        if out_channels % groups != 0:\n",
    "            raise ValueError('out_channels must be divisible by groups')\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.transposed = transposed\n",
    "        self.output_padding = output_padding\n",
    "        self.groups = groups\n",
    "        self.padding_mode = padding_mode\n",
    "        if transposed:\n",
    "            self.weight = Parameter(torch.Tensor(\n",
    "                in_channels, out_channels // groups, *kernel_size))\n",
    "        else:\n",
    "            self.weight = Parameter(torch.Tensor(\n",
    "                out_channels, in_channels // groups, *kernel_size))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "# ---------------------\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "# ---------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear模块中参数的默认初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BatchNorm2d模块的默认初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        if self.affine:\n",
    "            init.ones_(self.weight)\n",
    "            init.zeros_(self.bias)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `init.kaiming_uniform_`函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def kaiming_uniform_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu'):\n",
    "    fan = _calculate_correct_fan(tensor, mode)\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    std = gain / math.sqrt(fan)\n",
    "    bound = math.sqrt(3.0) * std  # Calculate uniform bounds from standard deviation\n",
    "    with torch.no_grad():\n",
    "        return tensor.uniform_(-bound, bound)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def _calculate_fan_in_and_fan_out(tensor):\n",
    "    dimensions = tensor.dim()\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "\n",
    "    if dimensions == 2:  # Linear\n",
    "        fan_in = tensor.size(1)\n",
    "        fan_out = tensor.size(0)\n",
    "    else:\n",
    "        num_input_fmaps = tensor.size(1)\n",
    "        num_output_fmaps = tensor.size(0)\n",
    "        receptive_field_size = 1\n",
    "        if tensor.dim() > 2:\n",
    "            receptive_field_size = tensor[0][0].numel()\n",
    "        fan_in = num_input_fmaps * receptive_field_size\n",
    "        fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以参考我们前言中初始化，就是这个源代码的简化版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练参数的其他函数\n",
    "\n",
    "1. `zero_grad(self)`\n",
    "2. `requires_grad_(self, requires_grad=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 子模块非训练参数管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不参与训练的参数\n",
    "    - requires_grad=False的Tesnor参数；\n",
    "- 这个在Module中称为buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    buffers(self, recurse=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    named_buffers(self, prefix='', recurse=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module中其他函数  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU运算切换函数\n",
    "\n",
    "1. `cuda(self, device=None)` \n",
    "2. `cpu(self)`\n",
    "3. `to(self, *args, **kwargs)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 类型转换函数\n",
    "\n",
    "1. `to(self, *args, **kwargs)`\n",
    "2. `half(self)`\n",
    "3. `float(self)`\n",
    "4. `double(self)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer与函数模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这两个模块本质都是Module，但是Layer有训练参数，所以会提供一个函数完成参数初始化：\n",
    "     - `reset_parameters(self)`\n",
    "         - 可以重载该函数实现参数的定制初始化，淡然可以通过apply函数指定初始化函数也行。\n",
    "     \n",
    "     - 同时，他们的构造器因为作用的缘故，也存在一些差异。\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch提供的Layer与函数清单\n",
    "\n",
    "- 这里只列出清单，因为使用与前面的函数一样，这里不累述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convolution layers\n",
    "2. Pooling layers\n",
    "3. Padding layers\n",
    "4. Normalization layers\n",
    "5. Recurrent layers\n",
    "6. Transformer layers\n",
    "7. Linear layers\n",
    "8. Dropout layers\n",
    "9. Sparse layers\n",
    "10. Vision layers\n",
    "11. Non-linear activations (weighted sum, nonlinearity)\n",
    "12. Non-linear activations (other)\n",
    "13. Distance functions\n",
    "14. Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前面已经说明优化器的使用模式非常简单"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过构造器设置\n",
    "\n",
    "```python\n",
    "    CLASS torch.optim.Optimizer(params, defaults)\n",
    "```\n",
    "\n",
    "- 参数说明：\n",
    "    - params：来自Module的parameters函数返回的迭代器\n",
    "    - defaults：使用字典管理的参数：\n",
    "        - 典型的是学习率\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数管理的函数\n",
    "\n",
    "1. 添加参数组：`add_param_group(param_group)`\n",
    "2. 加载状态字典：`load_state_dict(state_dict)`\n",
    "3. 返回状态字典：`state_dict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器的核心函数\n",
    "\n",
    "1. 更新梯度：`step(closure)`\n",
    "2. 清空梯度：`zero_grad()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `torch.optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)`\n",
    "2. `torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0)`\n",
    "3. `torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)`\n",
    "4. `torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)`\n",
    "5. `torch.optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)`\n",
    "6. `torch.optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)`\n",
    "7. `torch.optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)`\n",
    "8. `torch.optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None)`\n",
    "9. `torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)`\n",
    "10. `torch.optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50))`\n",
    "11. `torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一个特殊用途的Module\n",
    "\n",
    "- 在Torch提供了Sequential来组合多个Layer与函数。\n",
    "    - 前面的输出需要匹配后面的输出。下面是一个简单的例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1,20,5),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(20,64,5),、\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 说明\n",
    "\n",
    "1. 在torch中还提供了学习率修正的实现，这里暂时进一步说明，在后面高级主题中说明。\n",
    "2. 还与Module有关的就是参数初始化函数的实现，这里也不详细介绍，相对比较简单\n",
    "    - 请参考`torch.nn.init`模块中函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "259px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
