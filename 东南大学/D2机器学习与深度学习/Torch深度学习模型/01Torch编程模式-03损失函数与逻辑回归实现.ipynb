{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 逻辑回归与线性回归是有区别的，只要用来解决分类的问题，逻辑回归把从二分类基本模型分析，通过概率的方式来区分两个类：\n",
    "    - 属于A的概率，属于B的概率，哪个可能概率大就属于哪一类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 逻辑回归的决策模型，使用的还是是线性模型：\n",
    "    - $y = xW +b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为了从形式上接近概率，采用了一个概率密度函数（逻辑分布密度函数）\n",
    "    - $sigmoid(x) = \\dfrac{1}{1 + e^{-x}}$\n",
    "- 从而决策模型为：\n",
    "    - $y = sigmoid(xW +b) = \\dfrac{1}{1 + e^{-(xW + b)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 逻辑回归中，损失函数是：     \n",
    "    - $Loss =\\sum\\limits_{i \\in \\text{数据集}}y_i\\ ln(h(x_i)) +(1-y_i)\\ln(1-h(x_i))$      \n",
    "        - 上述公式被称为交叉熵（Cross   Entropy）    \n",
    "        - 其中$h(x_i)=sigmoid(x_iW + b)= \\dfrac{1}{1+e^{-(x_iW + b)}}$         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归PyTorch实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 采用PyTorch实现，我们还是使用梯度下降方法实现。\n",
    "\n",
    "- 首先认识下损失函数的表示。\n",
    "    - 损失函数的表达与决策函数有关。\n",
    "- 数据集：\n",
    "    - 采用鸢尾花数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策函数的表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 线性函数在torch中已经定义：linear函数；\n",
    "- sigmoid函数在torch中已经定义：sigmoid函数；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "torch.Size([100, 1]) torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "\n",
    "x = torch.Tensor(data[0:100])   # 取前面100个数据样本 \n",
    "y = torch.Tensor(target[0:100]).view(100,1) # 形状与x线性运算后的形状一样\n",
    "\n",
    "w = torch.randn(1, 4)    # 注意形状(linear会自动转置) \n",
    "b = torch.randn(1)        # w，b是可训练的，就是需要求导或者梯度\n",
    "\n",
    "y_ = torch.nn.functional.linear(input=x, weight=w, bias=b)\n",
    "print(y_.shape)\n",
    "sy_ = torch.sigmoid(y_)\n",
    "print(sy_.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数的表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 逻辑回归使用的损失函数是交叉熵，在Torch中也封装了一个函数：\n",
    "    - 其他损失函数后面专门用一节作为主题介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary_cross_entropy函数\n",
    "\n",
    "- 对数损失函数，没有做逻辑分布函数（sigmoid）运算\n",
    "\n",
    "```python\n",
    "    torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "- 参数说明：\n",
    "    - input：就是计算出来的y_\n",
    "    - target：就是原来数据集中标签y\n",
    "    - reduction: 损失的计算方式：\n",
    "        - 均值\n",
    "        - 求和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary_cross_entropy_with_logits函数\n",
    "\n",
    "- 自动做逻辑分布函数（sigmoid函数）运算。\n",
    "\n",
    "```python\n",
    "    torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3298)\n",
      "tensor(32.9785)\n",
      "tensor(0.3298)\n",
      "tensor(32.9785)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "\n",
    "x = torch.Tensor(data[0:100])   # 取前面100个数据样本 \n",
    "y = torch.Tensor(target[0:100]).view(100, 1) # 形状与x线性运算后的形状一样\n",
    "\n",
    "w = torch.randn(1, 4)    # 注意形状(linear会自动转置) \n",
    "b = torch.randn(1)        # w，b是可训练的，就是需要求导或者梯度\n",
    "\n",
    "y_ = torch.nn.functional.linear(input=x, weight=w, bias=b)\n",
    "sy_ = torch.sigmoid(y_)\n",
    "\n",
    "loss_mean = torch.nn.functional.binary_cross_entropy_with_logits(y_, y, reduction=\"mean\")    # 多一个运算，除以样本个数\n",
    "print(loss_mean)\n",
    "loss_sum = torch.nn.functional.binary_cross_entropy_with_logits(y_, y, reduction=\"sum\")\n",
    "print(loss_sum)\n",
    "\n",
    "# 自己手工做sigmoid运算\n",
    "loss_mean = torch.nn.functional.binary_cross_entropy(sy_, y, reduction=\"mean\")    # 多一个运算，除以样本个数\n",
    "print(loss_mean)\n",
    "loss_sum = torch.nn.functional.binary_cross_entropy(sy_, y, reduction=\"sum\")\n",
    "print(loss_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归分类实现\n",
    "\n",
    "- 下面的实现方法，没有使用随机梯度，实现的核心关键是：\n",
    "    1. 迭代梯度计算\n",
    "    2. 数据预测分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "误差损失值：  8.051266，\t准确率为：   50.00%\n",
      "误差损失值：  0.370522，\t准确率为：   95.00%\n",
      "误差损失值：  0.295042，\t准确率为：   95.00%\n",
      "误差损失值：  0.252245，\t准确率为：   95.00%\n",
      "误差损失值：  0.224739，\t准确率为：   96.00%\n",
      "误差损失值：  0.205526，\t准确率为：   96.00%\n",
      "误差损失值：  0.191302，\t准确率为：   96.00%\n",
      "误差损失值：  0.180313，\t准确率为：   97.00%\n",
      "误差损失值：  0.171543，\t准确率为：   97.00%\n",
      "误差损失值：  0.164363，\t准确率为：   97.00%\n",
      "训练完毕！\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "\n",
    "x = torch.Tensor(data[50:150])   # 取前面100个数据样本 （前面50与后面的两个50是可分的，后面两个50线性可分性差点）\n",
    "y = torch.Tensor(target[0:100]).float().view(100, 1) # 形状与x线性运算后的形状一样\n",
    "\n",
    "w = torch.randn(1, 4)    \n",
    "b = torch.randn(1)        \n",
    "\n",
    "w.requires_grad = True   # 可训练（x,y是不需要训练的）\n",
    "b.requires_grad = True\n",
    "\n",
    "epoch = 10000\n",
    "learn_rate = 0.01\n",
    "\n",
    "for n in range(epoch):\n",
    "    # forward：决策模型表示\n",
    "    y_ = torch.nn.functional.linear(input=x, weight=w, bias=b)   # 计算线性输出\n",
    "    sy_ = torch.sigmoid(y_)    # 计算逻辑分布运算（输出的值可以作为概率使用）\n",
    "    # loss：损失函数表示\n",
    "    loss_mean = torch.nn.functional.binary_cross_entropy(sy_, y, reduction=\"mean\")\n",
    "    # backward：计算梯度\n",
    "    loss_mean.backward()\n",
    "    \n",
    "    # 更新梯度\n",
    "    with torch.autograd.no_grad():   # 关闭梯度计算跟踪\n",
    "        w -= learn_rate * w.grad     # 更新权重梯度\n",
    "        w.grad.zero_()     # 清空本次计算的梯度（因为梯度是累加计算，不清空就累加）\n",
    "        b -= learn_rate * b.grad     # 更新偏置项梯度\n",
    "        b.grad.zero_()   \n",
    "        # 观察训练过程中的损失下降，与训练集预测的准确性\n",
    "        if n % 1000 == 0:\n",
    "            print(F\"误差损失值：{loss_mean:10.6f}，\", end=\"\")\n",
    "            sy_[sy_ > 0.5] = 1\n",
    "            sy_[sy_ <= 0.5] = 0\n",
    "\n",
    "            correct_rate = (sy_ ==  y).float().mean()     # 逻辑值在Torch不给计算平均值，所以需要转换为float类型\n",
    "            print(F\"\\t准确率为：{correct_rate*100: 8.2f}%\")\n",
    "    \n",
    "print(\"训练完毕！\")   # 下面输出的结果与sklearn与tensorflow训练的结果一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch提供的损失函数有：\n",
    "    1. binary_cross_entropy\n",
    "    2. binary_cross_entropy_with_logits\n",
    "    3. poisson_nll_loss\n",
    "    4. cosine_embedding_loss\n",
    "    5. cross_entropy\n",
    "    6. ctc_loss\n",
    "    7. hinge_embedding_loss\n",
    "    8. kl_div\n",
    "    9. l1_loss\n",
    "    10. mse_loss\n",
    "    11. margin_ranking_loss\n",
    "    12. multilabel_margin_loss\n",
    "    13. multilabel_soft_margin_loss\n",
    "    14. multi_margin_loss\n",
    "    15. nll_loss\n",
    "    16. smooth_l1_loss\n",
    "    17. soft_margin_loss\n",
    "    18. triplet_margin_loss\n",
    "\n",
    "- 这些函数都有标准的数学公式，每个函数的使用方式都一样，下面直接列出公式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## binary_cross_entropy与binary_cross_entropy_with_logits函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这两个函数的本质是一样的，区别在于带后缀的with_logits函数对input数据多一个sigmoid操作。\n",
    "    - sigmoid函数也称logits函数。\n",
    "    - 主要用于二分类，比如典型的逻辑回归，例子见上面；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## poisson_nll_loss函数\n",
    "\n",
    "- 泊松负对数似然损失（Poisson negative log likelihood loss）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 泊松分布的函数为：\n",
    "    - $P(x=k)= \\dfrac{\\lambda ^ {k}}{k!} e ^{- \\lambda}$\n",
    "        - $\\lambda$表示单位时间内随机事件发生的次数；\n",
    "        - 泊松分布的期望与方差都为$\\lambda$；\n",
    "        - $k!$是$k$的阶乘；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 泊松分布与二项分布的关系：\n",
    "    - 泊松分布是由二项分布推导而来，当二项分布的n很大，p很小的时候，泊松分布可以作为二项分布的近似，这时$\\lambda=np$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数定义：\n",
    "\n",
    "```python\n",
    "    torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "- 参数：\n",
    "    - log_input：逻辑值，用来设置是否对输入做exp指数运算：\n",
    "        - False：$exp^{input} - target * input$\n",
    "        - True：$input - target * log(input + eps)$ ：其中eps是无穷小量，用来防止input为0的情况。\n",
    "    - full：是否添加Stirling近似项\n",
    "        - $target * log(target) - target + 0.5 * log(2 * \\pi * target)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 损失函数计算公式：\n",
    "    - target数据服从泊松分布；\n",
    "    - $loss = \\sum \\limits _{i} ( e^{\\bar{y_i}} - y_i * \\bar{y_i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 例子代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0067)\n",
      "tensor(1.0067)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "\n",
    "x = torch.Tensor(data[0:100])   # 取前面100个数据样本 \n",
    "y = torch.Tensor(target[0:100]).view(100, 1) # 形状与x线性运算后的形状一样\n",
    "\n",
    "w = torch.randn(1, 4)    # 注意形状(linear会自动转置) \n",
    "b = torch.randn(1)        # w，b是可训练的，就是需要求导或者梯度\n",
    "\n",
    "y_ = torch.nn.functional.linear(input=x, weight=w, bias=b)\n",
    "sy_ = torch.sigmoid(y_)\n",
    "\n",
    "loss = torch.nn.functional.poisson_nll_loss(sy_, y)   # 默认均值：比较常采用\n",
    "print(loss)\n",
    "\n",
    "# 手工计算的效果（log_input = True）\n",
    "loss_manual = sy_.exp() - y * sy_\n",
    "loss_manual = loss_manual.mean()\n",
    "print(loss_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosine_embedding_loss函数\n",
    "\n",
    "- 用来度量两个向量是否相似。主要用于半监督学习与学习非线性嵌入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数定义：\n",
    "\n",
    "```python\n",
    "    torch.nn.functional.cosine_embedding_loss(\n",
    "        input1,     # 向量1\n",
    "        input2,     # 向量2\n",
    "        target,     # 标签\n",
    "        margin=0, size_average=None, reduce=None, reduction='mean') → Tensor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算公式：\n",
    "    - $loss(x,y) = \\begin{cases} 1-cos(x_1, x_2)& 如 y =1\\\\ max(0, cos(x_1, x_2)-margin )&如y=-1 \\\\   \\end{cases}$\n",
    "        - 其中：$cos(x_1, x_2)$是向量$x_1,x_2$夹角的余弦。\n",
    "        - margin的取值范围`[-1, 1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nll_loss与cross_entropy函数\n",
    "\n",
    "- 负对数似然损失函数。用来做C个类别的分类损失函数。\n",
    "- 实际上这两个函数本质是一样的。\n",
    "    - cross_entropy多做了log_softmax运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数定义：\n",
    "\n",
    "```python\n",
    "    torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
    "```\n",
    "\n",
    "- 参数说明：\n",
    "    - weight是一个1-D的张量（Tensor），张量的长度与类别数相同，用来加权每个分类的类别。\n",
    "    - input：是2-D数据（N，C）：N表示数量，C表示分类类别；\n",
    "    - target：是1-D数据，长度为N即可。\n",
    "    \n",
    "- 提示：\n",
    "    - 因为是多分类问题，所以对于输出的结果应该是one-hot，比如2的one-hot就是`[0,0,1,0,0,0]`，假设最大标签是5。\n",
    "    - 所以target必须是LongTensor类型的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 例子代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-9.1227)\n",
      "tensor(7.1684)\n",
      "tensor(7.1684)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "\n",
    "x = torch.Tensor(data)   #  （150，4）\n",
    "y = torch.Tensor(target).long()   # （150）\n",
    "\n",
    "w = torch.randn(3, 4)    # 注意形状(linear会自动转置) ：3表示类别数据（输出的长度）\n",
    "b = torch.randn(3)        # w，b是可训练的，就是需要求导或者梯度\n",
    "\n",
    "y_ = torch.nn.functional.linear(input=x, weight=w, bias=b)\n",
    "sy_ = y_.log_softmax(dim=1)\n",
    "# print(sy_.shape)\n",
    "\n",
    "loss = torch.nn.functional.nll_loss(y_, y)   # 默认均值：比较常采用\n",
    "print(loss)\n",
    "loss = torch.nn.functional.nll_loss(sy_, y)   #\n",
    "print(loss)\n",
    "\n",
    "# cross_entropy交叉熵函数(多运算了一个sigmoid运算)\n",
    "loss_mamual = torch.nn.functional.cross_entropy(y_, y)     # nll_loss就是cross_entrop编码，自动采用softmax的one-hotb\n",
    "print(loss_mamual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross_entropy的补充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cross_entropy函数的计算公式是：\n",
    "    - $loss(x, target) = -log(  \\dfrac{e ^ {x[target]} }{ \\sum \\limits _i e ^ {x[i]}}  ) = -x[target] + log(\\sum  \\limits _i e ^{x[i]})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 下面就是cross_entropy的手工实现函数：\n",
    "     - 只使用了一个样本测试，类别是5个类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入的数据集： tensor([[1., 2., 3., 4., 5.]])\n",
      "输出的数据集： tensor([2])\n",
      "cross_entropy函数输出的结果： tensor(2.4519)\n",
      "手工计算结果： tensor(2.4519)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "data_input = torch.FloatTensor([[1.0, 2.0, 3.0, 4.0, 5.0]])  # 计算的y,\n",
    "\n",
    "data_target = torch.LongTensor([2])  # 改下标不能超过上面的维数-1，这是损失函数的计算过程决定的\n",
    "\n",
    "loss_out = torch.nn.functional.cross_entropy(data_input,  data_target)\n",
    "print(\"输入的数据集：\", data_input)\n",
    "print(\"输出的数据集：\", data_target)\n",
    "print(\"cross_entropy函数输出的结果：\", loss_out)\n",
    "\n",
    "# 下面是交叉熵函数的手工计算过程\n",
    "\n",
    "result_1 = 0.0\n",
    "\n",
    "# 计算第一部分：−𝑥[𝑡𝑎𝑟𝑔𝑒𝑡]\n",
    "for row in range(data_input.size()[0]):    # 行循环(表示样本与对应的标签),这里其实是1\n",
    "    result_1 -= data_input[row][data_target[row]]\n",
    "\n",
    "result_2 = 0.0\n",
    "\n",
    "# 计算第二部分：∑𝑒𝑥[𝑖]\n",
    "for row in range(data_input.size()[0]):    # 行循环(表示样本与对应的标签),这里其实是1\n",
    "    for col in range(data_input.size()[1]):\n",
    "        result_2 += math.exp(data_input[row][col])\n",
    "\n",
    "# 最终的结果\n",
    "print(\"手工计算结果：\", result_1 + math.log(result_2))   #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nll_loss函数的补充\n",
    "\n",
    "- nll_loss函数名字叫负对数似然函数，实际上根本没有做任何对数运算，其计算公式如下：\n",
    "     - $loss(x, target) = x[target]$\n",
    "     \n",
    "     - 直接把x当成对数概率，并最终取`x[target]`作为这个类别的损失，最后的损失就是所有样本的损失。\n",
    "     \n",
    "- 注意：\n",
    "    - 一般nll_loss会与log_softmax一起使用，本质也就等于cross_entropy损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0533)\n",
      "tensor(0.0533)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "\n",
    "x = torch.Tensor(data)   #  （150，4）\n",
    "y = torch.Tensor(target).long()   # （150）\n",
    "\n",
    "w = torch.randn(3, 4)    # 注意形状(linear会自动转置) ：3表示类别数据（输出的长度）\n",
    "b = torch.randn(3)        # w，b是可训练的，就是需要求导或者梯度\n",
    "\n",
    "y_ = torch.nn.functional.linear(input=x, weight=w, bias=b)\n",
    "# sy _ = y_.log_softmax(dim=1)\n",
    "sy_ = y_ \n",
    "loss = torch.nn.functional.nll_loss(sy_, y)   \n",
    "print(loss)\n",
    "\n",
    "# 手工计算\n",
    "y_one = torch.nn.functional.one_hot(y).float()    # 做了个单热编码，方便矩阵运算，否则就要取下标。\n",
    "re = sy_ *  y_one\n",
    "# re = re.log()\n",
    "re = -re\n",
    "re =re.sum(dim=1)\n",
    "print(re.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mse_loss损失函数\n",
    "\n",
    "- 最直观的损失函数：均方差损失，计算公式如下：\n",
    "    - $loss(x, target) = \\dfrac{1}{N} \\sum  \\limits _{i \\in N} (x_i - target_i) ^ 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数说明：\n",
    "\n",
    "```python\n",
    "    torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 例子代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.2841)\n",
      "tensor(14.2841)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "\n",
    "x = torch.Tensor(data)   #  （150，4）\n",
    "y = torch.Tensor(target).view(150,1)   # （150）\n",
    "\n",
    "w = torch.randn(1, 4)    # 注意形状(linear会自动转置) ：3表示类别数据（输出的长度）\n",
    "b = torch.randn(1)        # w，b是可训练的，就是需要求导或者梯度\n",
    "\n",
    "y_ = torch.nn.functional.linear(input=x, weight=w, bias=b)\n",
    "\n",
    "loss = torch.nn.functional.mse_loss(y_, y)   \n",
    "print(loss)\n",
    "\n",
    "# 手工计算\n",
    "loss_manual = ((y -y_) ** 2).mean()\n",
    "print(loss_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l1_loss函数\n",
    "\n",
    "- 这个函数从字面上理解，应该是L1范数度量的距离误差，与均方差损失函数属于同一性质的损失函数。函数公式为：\n",
    "    - $loss(x, target) = \\dfrac{1}{N} \\sum \\limits _{i \\in N} | x_i - target_i |$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数的定义\n",
    "\n",
    "```python\n",
    "    torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6954)\n",
      "tensor(0.6954)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "\n",
    "x = torch.Tensor(data)   #  （150，4）\n",
    "y = torch.Tensor(target).view(150, 1) # （150, 1）\n",
    "\n",
    "w = torch.randn(1, 4)    # 注意形状(linear会自动转置) ：3表示类别数据（输出的长度）\n",
    "b = torch.randn(1)        # w，b是可训练的，就是需要求导或者梯度\n",
    "\n",
    "y_ = torch.nn.functional.linear(input=x, weight=w, bias=b)  \n",
    "sy_ = y_.sigmoid()    \n",
    "# sy_ = y_\n",
    "\n",
    "loss = torch.nn.functional.l1_loss(sy_, y)   # \n",
    "print(loss)\n",
    "\n",
    "# 手工计算\n",
    "loss_manual = (y - sy_).abs().mean()\n",
    "print(loss_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kl_div函数\n",
    "\n",
    "- The `Kullback-Leibler divergence`_ Loss.\n",
    "    - 也称KL距离，一种不同于几何距离的度量方式，用来度量两个概率的差异的距离。\n",
    "\n",
    "    - > &emsp;&emsp;相对熵（relative entropy），又被称为Kullback-Leibler散度（Kullback-Leibler divergence）或信息散度（information divergence），是两个概率分布（probability distribution）间差异的非对称性度量  。\n",
    "    - > &emsp;&emsp;在在信息理论中，相对熵等价于两个概率分布的信息熵（Shannon entropy）的差值。\n",
    "    - >&emsp;&emsp;相对熵是一些优化算法，例如最大期望算法（Expectation-Maximization algorithm, EM）的损失函数  。此时参与计算的一个概率分布为真实分布，另一个为理论（拟合）分布，相对熵表示使用理论分布拟合真实分布时产生的信息损耗 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算公式：\n",
    "    - 信息熵：$H(x) = - \\sum \\limits _{i=1} ^ N p(x_i) log \\ p(x_i)$\n",
    "    - 散度：$D_{KL}(p | q) = \\sum \\limits _{i=1} ^ N p(x_i) (log \\ p(x_i) - log \\  q(x_i)) = \\sum \\limits _{i=1} ^ N p(x_i) log \\dfrac{p(x_i)}{q(x_i)} $\n",
    "    \n",
    "    - Torch中封装的公式：\n",
    "        - $loss(x, target) = target (log (target) - x)$：target=0的情况总体看成0，只考虑target为1的情况\n",
    "        - $loss(x, target) = target (- x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数定义\n",
    "\n",
    "```python\n",
    "    torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean')\n",
    "    \n",
    "```\n",
    "\n",
    "- 参数说明：\n",
    "    - reduction参数：batchmean最后的均值使用batch_size，mean使用输出的个数；\n",
    "        - 注意batch_size与输出总数是有差别的。如果是（N，1）维度，则没有差别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 例子代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.9451)\n",
      "tensor(-0.9451)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "\n",
    "x = torch.Tensor(data[:100])   #  （150，4）\n",
    "y = torch.Tensor(target[:100]).view(100, 1) # （150, 1）\n",
    "\n",
    "w = torch.randn(1, 4)    # 注意形状(linear会自动转置) ：3表示类别数据（输出的长度）\n",
    "b = torch.randn(1)        # w，b是可训练的，就是需要求导或者梯度\n",
    "\n",
    "y_ = torch.nn.functional.linear(input=x, weight=w, bias=b)  \n",
    "sy_ = y_\n",
    "loss = torch.nn.functional.kl_div(sy_, y, reduction=\"batchmean\")   \n",
    "print(loss)\n",
    "\n",
    "# 手工计算（本质与nll_loss函数一样：nll_loss支持多类）\n",
    "loss_manual = - y * (sy_)\n",
    "print(loss_manual.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hinge_embedding_loss函数\n",
    "\n",
    "- 用来测试两个输入的数据是否相似。\n",
    "    - y的取值为-1或者1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数定义\n",
    "\n",
    "```python \n",
    "    torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') → Tensor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 函数公式：\n",
    "    - $loss(x,y) = \\begin{cases} x& 如 y =1\\\\ max(0, 1 -x )&如y=-1 \\\\   \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 例子代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3959)\n",
      "tensor(0.3959)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import torch\n",
    "\n",
    "data, target = sklearn.datasets.load_iris(return_X_y=True)\n",
    "target[target == 0] = -1 \n",
    "x = torch.Tensor(data[:100])   #  （150，4）\n",
    "y = torch.Tensor(target[:100]).view(100, 1) # （150, 1）\n",
    "w = torch.randn(1, 4)    # 注意形状(linear会自动转置) ：3表示类别数据（输出的长度）\n",
    "b = torch.randn(1)        # w，b是可训练的，就是需要求导或者梯度\n",
    "\n",
    "y_ = torch.nn.functional.linear(input=x, weight=w, bias=b)  \n",
    "sy_ = y_\n",
    "# print(y_)\n",
    "# sy_ = y_.sigmoid()\n",
    "loss = torch.nn.functional.hinge_embedding_loss(sy_, y, reduction=\"mean\")   # 默认均值：比较常采用（多一个sigmoid运算）\n",
    "print(loss)\n",
    "\n",
    "# 手工计算（本质与nll_loss函数一样：nll_loss支持多类）\n",
    "loss_manual[0:50] = 1 - sy_[0:50]\n",
    "loss_manual[loss_manual< 0] = 0\n",
    "\n",
    "loss_manual[50:100] = sy_[50:100]\n",
    "# loss_manual[loss_manual <0] = 0\n",
    "print(loss_manual.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 其他损失函数是基于多分类与其他目的的变种函数，这些损失函数在特定的需求理解会更加容易。\n",
    "    - 比如：soft_margin_loss是基于SVM的软距离提出的一种损失优化方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
